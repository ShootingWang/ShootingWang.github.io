<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
<meta name="baidu-site-verification" content="GLZSr3LDqT" />
<meta name="google-site-verification" content="fZvQBljM9I6UA4l4wwiA5wECEn7k0V4-DgCfWEPNtFc" />
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon2.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon2.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"github.com","root":"/","scheme":"Pisces","version":"8.0.0-rc.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"valine","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

<link rel="stylesheet" href="/js/prism/prism.css">

<script src="http://echarts.baidu.com/dist/echarts.common.min.js"></script>
  <meta property="og:type" content="article">
<meta property="og:title" content="Data Scientist | Spark Learning">
<meta property="og:url" content="https://github.com/ShootingWang/ShootingWang.github.io/Spark-Learning.html">
<meta property="og:site_name" content="Skye">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-08-25T14:59:15.000Z">
<meta property="article:modified_time" content="2022-10-12T04:44:21.720Z">
<meta property="article:author" content="Skye">
<meta property="article:tag" content="Data Science">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://github.com/ShootingWang/ShootingWang.github.io/Spark-Learning.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Data Scientist | Spark Learning | Skye</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Skye" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><script type="text/javascript" color="0,0,0" opacity='0.5' zIndex="-1" count="99" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
  </script>
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line toggle-line-first"></span>
        <span class="toggle-line toggle-line-middle"></span>
        <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Skye</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页 Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于 About Me</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签 Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类 Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档 Archives</a>

  </li>
        <li class="menu-item menu-item-update">

    <a href="/update/" rel="section"><i class="fa fa-list-ul fa-fw"></i>更新 Update</a>

  </li>
        <li class="menu-item menu-item-tool">

    <a href="/tools/" rel="section"><i class="fa fa-tools fa-fw"></i>工具 Tools</a>

  </li>
        <li class="menu-item menu-item-treasure">

    <a href="/treasure/" rel="section"><i class="fa fa-gem fa-fw"></i>资源 Treasure</a>

  </li>
        <li class="menu-item menu-item-pool">

    <a href="/pool/" rel="section"><i class="fa fa-book fa-fw"></i>题库 Question Bank</a>

  </li>
        <li class="menu-item menu-item-links">

    <a href="/links" rel="section"><i class="fa fa-link fa-fw"></i>友链 Links</a>

  </li>
        <li class="menu-item menu-item-timeline">

    <a href="/timeline/" rel="section"><i class="fa fa-cube fa-fw"></i>随笔 Essay</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索 Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/ShootingWang" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

    <!-- echarts -->
    <script type="text/javascript" src="/js/src/echarts.common.js"></script>
    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://github.com/ShootingWang/ShootingWang.github.io/Spark-Learning.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Skye">
      <meta itemprop="description" content="刻苦，沉着，精进不休">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Skye">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Data Scientist | Spark Learning
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-08-25 22:59:15" itemprop="dateCreated datePublished" datetime="2020-08-25T22:59:15+08:00">2020-08-25</time>
            </span>

          
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-10-12 12:44:21" itemprop="dateModified" datetime="2022-10-12T12:44:21+08:00">2022-10-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Data-Analysis-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" itemprop="url" rel="index"><span itemprop="name">Data Analysis 数据分析</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/Spark-Learning.html#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Spark-Learning.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <center></center>
<span id="more"></span>
<h1 id="Spark"><a class="header-anchor" href="#Spark"></a>Spark</h1>
<p>Spark包括SQL和处理结构化数据的库（Spark SQL）、机器学习库（MLlib）、流处理库（Spark Streaming和较新的结构化流式处理），以及图分析（GraphX）的库。</p>
<p>此外还有数百种开源外部库：</p>
<ul>
<li>用于各种存储系统的连接器</li>
<li>机器学习算法</li>
<li>……</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://spark-packages.org/">spark-package.org</a>提供了外部库的索引。</p>
<p>Spark支持：</p>
<ul>
<li>批处理应用程序（基于函数式编程的API）</li>
<li>交互式数据处理和即席（ad-hoc）查询（将Scala解释器插入Spark，可以提供一个高可用的交互式系统，用于在数百台机器上运行查询）</li>
<li>Shark系统：可以在Spark上运行SQL查询并满足分析师与数据科学家的交互式应用的引擎</li>
</ul>
<h2 id="Spark四大组件"><a class="header-anchor" href="#Spark四大组件"></a>Spark四大组件</h2>
<h3 id="Spark-Streaming"><a class="header-anchor" href="#Spark-Streaming"></a>Spark Streaming</h3>
<p>Spark Streaming是Spark平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API</p>
<h3 id="Spark-SQL"><a class="header-anchor" href="#Spark-SQL"></a>Spark SQL</h3>
<p>Spark SQL是Spark用来操作结构化数据的组件</p>
<ul>
<li>通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL语言（HQL）来查询数据</li>
<li>Spark SQL支持多种数据源类型，例如Hive表、Parquet以及JSON等</li>
<li>用户可以在单个的应用中同时进行SQL查询和复杂的数据分析</li>
</ul>
<h3 id="GraphX"><a class="header-anchor" href="#GraphX"></a>GraphX</h3>
<p>GraphX是Spark面向图计算提供的框架与算法库。</p>
<h3 id="MLlib"><a class="header-anchor" href="#MLlib"></a>MLlib</h3>
<p>MLlib是Spark提供的一个机器学习算法库，其中包含了多种经典、常见的机器学习算法，主要有分类、回归、聚类、协同过滤等。</p>
<h1 id="运行"><a class="header-anchor" href="#运行"></a>运行</h1>
<ul>
<li>可以使用Python、Java、Scala、R或SQL等语言来运行Spark</li>
<li>Spark本身是用Scala编写的，并且运行在Java虚拟机（JVM）上</li>
</ul>
<p>可以使用2种方法来运行Spark：</p>
<ol>
<li>在电脑上下载并安装Apache Spark
<ol>
<li>安装Java、Python</li>
<li>在<a target="_blank" rel="noopener" href="http://spark.apache.org/downloads.html">官网</a>选择“Pre-built for Hadoop 3.2 and later”，单击“Download Spark”</li>
<li>解压下载的文件</li>
</ol>
</li>
<li>在Databricks Community Edition<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>上运行基于Web的版本
<ol>
<li>按照<a href="https://github.com/databricks/Spark-The-Definitive-Guide">Spark:The Definitive Guide</a>中的操作说明，通过Web界面使用Scala、Python、SQL或R来运行Spark程序，还可以将得到的处理结果可视化</li>
</ol>
</li>
</ol>
<h1 id="交互式控制台"><a class="header-anchor" href="#交互式控制台"></a>交互式控制台</h1>
<p>Spark支持不同编程语言的交互式控制台：</p>
<ul>
<li>Python</li>
<li>Scala</li>
<li>SQL</li>
</ul>
<h2 id="Python控制台"><a class="header-anchor" href="#Python控制台"></a>Python控制台</h2>
<ul>
<li>需要安装Python 2或Python 3</li>
<li>在Spark的主目录下运行：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/pyspark</span><br></pre></td></tr></table></figure>
<p>然后输入<code>spark</code>并按回车键，将看到打印的<code>SparkSession</code>对象。</p>
<h2 id="Scala控制台"><a class="header-anchor" href="#Scala控制台"></a>Scala控制台</h2>
<ul>
<li>在Spark的主目录下运行：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-shell</span><br></pre></td></tr></table></figure>
<p>然后输入<code>spark</code>并按回车键，将看到打印的<code>SparkSession</code>对象。</p>
<h2 id="SQL控制台"><a class="header-anchor" href="#SQL控制台"></a>SQL控制台</h2>
<ul>
<li>在Spark的主目录下运行：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-sql</span><br></pre></td></tr></table></figure>
<h1 id="基本架构"><a class="header-anchor" href="#基本架构"></a>基本架构</h1>
<ul>
<li>Spark是一种管理和协调跨多台计算机的计算任务的软件框架</li>
</ul>
<h2 id="应用程序"><a class="header-anchor" href="#应用程序"></a>应用程序</h2>
<p>Spark应用程序由<u>一个驱动器进程</u>和<u>一组执行器进程</u>组成</p>
<ol>
<li><strong>一个驱动器进程</strong>
<ul>
<li>驱动器进程运行<code>main()</code>函数，位于集群中的一个节点上，负责：
<ul>
<li>维护Spark应用程序的相关信息</li>
<li>回应用户的程序或输入</li>
<li>分析任务并分发给若干执行器进行处理</li>
</ul>
</li>
<li>驱动器是Spark应用程序的核心，在应用程序执行的整个生命周期中维护着所有相关信息</li>
</ul>
</li>
<li><strong>一组执行器进程</strong>
<ul>
<li>执行器负责执行驱动器分配的实际计算工作</li>
<li>每个执行器只负责两件事：
<ul>
<li>执行由驱动器分配给它的代码</li>
<li>并将该执行器的计算状态报告给运行驱动器的节点</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>page 19的插图</p>
<ul>
<li>用户可以配置指定每个节点上运行多少个执行器</li>
<li>Spark还可本地运行，此时，驱动器和执行器只是简单的进程，可以位于同一台机器或不同的机器上</li>
<li>Spark使用一个集群管理器来跟踪可用的资源；集群管理器可以是3个核心集群管理器中的任意一个：
<ul>
<li>独立集群管理器</li>
<li>YARN</li>
<li>Mesos</li>
</ul>
</li>
</ul>
<h1 id="Spark-API"><a class="header-anchor" href="#Spark-API"></a>Spark API</h1>
<p>Spark有两套基本的API：</p>
<ul>
<li>低级“非结构化”API</li>
<li>更高级别的结构化API</li>
</ul>
<h2 id="多语言支持"><a class="header-anchor" href="#多语言支持"></a>多语言支持</h2>
<p>Spark API支持多种编程语言：</p>
<ul>
<li>Scala：Spark主要用Scala编写，Scala是Spark的默认语言</li>
<li>Java</li>
<li>Python：Python几乎支持所有Scala支持的结构</li>
<li>SQL：Spark支持ANSI SQL 2003标准的一个子集</li>
<li>R：Spark有两个常用的R库
<ol>
<li>SparkR：是Spark核心之一</li>
<li>sparklyr：R语言开源社区维护的包</li>
</ol>
</li>
</ul>
<p>page 21的插图</p>
<h2 id="SparkSession"><a class="header-anchor" href="#SparkSession"></a>SparkSession</h2>
<p>可以通过名为SparkSession的驱动器来控制Spark应用程序，需要创建一个SparkSession实例用来在集群中执行用户定义的操作</p>
<ul>
<li>每一个Spark应用程序都需要一个SparkSession与之对应</li>
<li>在Scala和Python中，启动控制台时，SparkSession就被实例化为一个名叫<code>spark</code>的对象</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line">spark</span><br><span class="line"></span><br><span class="line">res0: org.apache.spark.sql.<span class="type">SparkSession</span> = org.apache.spark.sql.<span class="type">SparkSession</span>@...</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## in python</span></span><br><span class="line">spark</span><br><span class="line"></span><br><span class="line"><span class="comment">## 返回</span></span><br><span class="line">&lt;pyspark.sql.session.SparkSession at <span class="number">0x7efda4c1ccd0</span>&gt;</span><br></pre></td></tr></table></figure>
<p>转换操作和动作操作的逻辑结构是DataFrame和Datset，执行一次转换操作都会创建一个新的DataFrame或Dataset，而动作操作则会触发计算，或将DataFrame和Dataset转换成本地语言类型。</p>
<h2 id="转换操作"><a class="header-anchor" href="#转换操作"></a>转换操作</h2>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line"><span class="comment">// 查找当前DataFrame中的所有偶数</span></span><br><span class="line"><span class="keyword">val</span> divisBy2 = myRange.where(<span class="string">&quot;number % 2 = 0&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## in python</span></span><br><span class="line"><span class="comment">## 查找当前DataFrame中的所有偶数</span></span><br><span class="line">divisBy2 = myRange.where(<span class="string">&quot;number % 2 = 0&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>这些转换并没有实际输出，只是抽象转换</li>
<li><strong>转换操作</strong>是使用Spark表达业务逻辑的核心</li>
</ul>
<p>有两类转换操作：</p>
<ol>
<li><strong>窄转换</strong>：指定窄依赖关系（narrow dependency）的转换操作
<ul>
<li>一对一映射</li>
<li>每个输入分区仅决定一个输出分区的转换</li>
<li>Spark将自动执行流水线处理——如果在DataFrame上指定了多个过滤操作，它们将全部在内存中执行</li>
<li>补充24页 插图</li>
</ul>
</li>
<li><strong>宽转换</strong>：指定宽依赖关系（wide dependency）的转换操作
<ul>
<li>一对多映射</li>
<li>每个输入分区决定了多个输出分区</li>
<li>也被称为<strong>洗牌（shuffle）操作</strong>，会在整个集群中执行互相交换分区数据的功能</li>
<li>当执行shuffle操作时，Spark将结果写入磁盘</li>
</ul>
</li>
</ol>
<h3 id="惰性评估"><a class="header-anchor" href="#惰性评估"></a>惰性评估</h3>
<p><strong>惰性评估</strong>（lazy evaluation）：等到绝对需要时才执行计算</p>
<p>在Spark中，当用户表达一些对数据的操作时，不是立即修改数据，而是建立一个作用到原始数据的转换计划。Spark首先会将这个计划编译为可以在集群中高效运行的流水线式的物理执行计划，然后等待，直到最后时刻才开始执行代码。</p>
<ul>
<li>可以优化整个从输入端到输出端的数据流</li>
</ul>
<blockquote>
<p>如：DataFrame的谓词下推（predicate pushdown）：假设构建了一个含有多个转换操作的Spark作业，并在最后指定了一个过滤操作，且这个过滤操作只需要数据源中的某一行数据，则最有效的方法就是在最开始仅访问我们需要的单个记录，Spark会通过自动下推这个过滤操作来优化整个物理执行计划</p>
</blockquote>
<h2 id="动作操作"><a class="header-anchor" href="#动作操作"></a>动作操作</h2>
<ul>
<li>运行一个动作操作（action）以触发计算</li>
<li>一个动作指示Spark在一系列转换操作后计算一个结果</li>
</ul>
<p>最简单的动作操作是：<code>count()</code>，计算一个DataFrame中的记录总数</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">divisBy2.<span class="built_in">count</span>()</span><br></pre></td></tr></table></figure>
<p>动作有三类：</p>
<ul>
<li>在控制台中查看数据的动作</li>
<li>在某个语言中将数据汇集为原生对象的动作</li>
<li>写入输出数据源的动作</li>
</ul>
<h2 id="Spark用户接口"><a class="header-anchor" href="#Spark用户接口"></a>Spark用户接口</h2>
<p>可以通过Spark的Web UI监控一个作业的进度</p>
<ul>
<li>Spark UI占用驱动器节点的4040端口</li>
<li>如果在本地模式下运行，可以通过<a target="_blank" rel="noopener" href="http://localhost:4040">http://localhost:4040</a> 访问Spark Web UI</li>
<li>Spark UI上显示了Spark作业的运行状态、执行环境和群集状态等信息</li>
</ul>
<p>一个Spark作业包含一系列转换操作，并由一个动作操作触发，并可以通过Spark UI监视该作业。</p>
<p>Spark的核心抽象：</p>
<ul>
<li>DataFrame</li>
<li>Dataset</li>
<li>SQL表</li>
<li>弹性分布式数据集（RDD，Resident Distributed Datasets）</li>
</ul>
<p>这些不同的抽象都表示分布式数据集合，其中最简单、最有效的是DataFrame，它支持所有语言。</p>
<h1 id="结构化API"><a class="header-anchor" href="#结构化API"></a>结构化API</h1>
<p>结构化API指以下三种核心分布式集合类型的API：</p>
<ol>
<li>（类型化的）Dataset类型</li>
<li>（非类型化的）DataFrame类型</li>
<li>SQL表和视图</li>
</ol>
<ul>
<li>大多数结构化API均适用于批处理和流处理</li>
<li>使用结构化API编写代码时，可以从批处理程序转换为流处理程序，反之亦然</li>
<li>结构化API是在编写大部分数据处理程序时会用到的基础抽象概念</li>
<li>Spark类型直接映射到不同语言API，并且针对Scala、Java、Python、SQL和R语言，都有一个对应的API查找表</li>
<li>即使通过Python或R语言来使用Spark结构化API，大多数情况下也是操作Spark类型而非Python类型</li>
</ul>
<p>DataFrame和Dataset是具有行和列类似于（分布式）数据表的集合类型。</p>
<ul>
<li>所有列的函数相同（可以用null来指定缺省值），并且某一列的类型必须在所有行中保持一致</li>
<li>Spark中的DataFrame和Dataset代表不可变的数据集合，可以通过它指定对特殊位置数据的操作（操作将以惰性评估方式执行）</li>
</ul>
<p><strong>行</strong>（Row）：一行对应一个数据记录</p>
<ul>
<li>DataFrame中的每条记录都必须是Row类型</li>
<li>可以通过SQL手动创建、从弹性分布式数据集（RDD）提取、从数据源手动创建这些行</li>
</ul>
<p><strong>列</strong>：表示一个简单类型（eg：整数、字符串），或者一个复杂类型（eg：数组、map映射），或者空值null</p>
<h2 id="核心对象"><a class="header-anchor" href="#核心对象"></a>核心对象</h2>
<h3 id="DataFrame"><a class="header-anchor" href="#DataFrame"></a>DataFrame</h3>
<p>DataFrame是最常见的结构化API，是包含行和列的数据表</p>
<ul>
<li>说明DataFrame的列和列类型的规则被称为<strong>模式</strong>（schema）</li>
<li>DataFrame与电子表格不同
<ul>
<li>电子表格位于一台计算机上，而Spark DataFrame可以跨越数千台计算机</li>
</ul>
</li>
<li>R DataFrame和Python DataFrame存在于一台机器上，而不是多台机器上</li>
<li>Spark可以将Python DataFrame或R DataFrame转换为Spark DataFrame</li>
<li>DataFrame实际是有类型的，只是Spark完全负责维护它们的类型，仅在运行时检查这些类型是否与schema中指定的类型一致</li>
<li>在Scala版本的Spark中，DataFrame是一些Row类型的Dataset的集合
<ul>
<li>Row类型：是Spark用于支持内存计算而优化的数据格式</li>
</ul>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line"><span class="comment">// 创建一组固定范围的数字</span></span><br><span class="line"><span class="comment">// DataFrame</span></span><br><span class="line"><span class="keyword">val</span> myRange = spark.range(<span class="number">1000</span>).toDF(<span class="string">&quot;number&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## in python</span></span><br><span class="line"><span class="comment">## 创建一组固定范围的数字</span></span><br><span class="line"><span class="comment">## DataFrame</span></span><br><span class="line">myRange = spark.<span class="built_in">range</span>(<span class="number">1000</span>).toDF(<span class="string">&quot;number&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Dataset"><a class="header-anchor" href="#Dataset"></a>Dataset</h3>
<ul>
<li>类型安全的结构化API，用于在Java和Scala中编写静态类型的代码</li>
<li>在Dataset上调用<code>collect</code>或<code>take</code>函数时，将会收集Dataset中合适类型的对象，而不是DataFrame的Row对象</li>
<li>Dataset仅适用于基于Java虚拟机（JVM）的语言（如Scala和Java），并通过case类或Java beans指定类型</li>
<li>Python版本和R语言版本的Spark并不支持Dataset，所有东西都是DataFrame</li>
</ul>
<h3 id="SQL表和视图"><a class="header-anchor" href="#SQL表和视图"></a>SQL表和视图</h3>
<p>表和视图与DataFrame基本相同，所以通常在DataFrame上执行SQL操作，而不是用DataFrame专用的执行代码</p>
<h2 id="执行"><a class="header-anchor" href="#执行"></a>执行</h2>
<p>结构化API执行过程<br>
61页<br>
待补充</p>
<h3 id="逻辑计划"><a class="header-anchor" href="#逻辑计划"></a>逻辑计划</h3>
<h3 id="物理计划"><a class="header-anchor" href="#物理计划"></a>物理计划</h3>
<h1 id="Spark类型"><a class="header-anchor" href="#Spark类型"></a>Spark类型</h1>
<h2 id="Java类型"><a class="header-anchor" href="#Java类型"></a>Java类型</h2>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用正确的Java类型</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</span><br><span class="line"><span class="type">ByteType</span> <span class="variable">x</span> <span class="operator">=</span> DataTypes.ByteType;</span><br></pre></td></tr></table></figure>
<p>Java类型参考表：</p>
<table>
<thead>
<tr>
<th style="text-align:left">数据类型</th>
<th style="text-align:left">值类型</th>
<th style="text-align:left">获取或创建数据类型的API</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">ByteType</td>
<td style="text-align:left">byte或Byte</td>
<td style="text-align:left"><code>DataTypes.ByteType</code></td>
</tr>
<tr>
<td style="text-align:left">ShortType</td>
<td style="text-align:left">short或Short</td>
<td style="text-align:left"><code>DataTypes.ShortType</code></td>
</tr>
<tr>
<td style="text-align:left">IntegerType</td>
<td style="text-align:left">int或Integer</td>
<td style="text-align:left"><code>DataTypes.IntegerType</code></td>
</tr>
<tr>
<td style="text-align:left">LongType</td>
<td style="text-align:left">long或Long</td>
<td style="text-align:left"><code>DataTypes.LongType</code></td>
</tr>
<tr>
<td style="text-align:left">FloatType</td>
<td style="text-align:left">float或Float</td>
<td style="text-align:left"><code>DataTypes.FloatType</code></td>
</tr>
<tr>
<td style="text-align:left">DoubleType</td>
<td style="text-align:left">double或Double</td>
<td style="text-align:left"><code>DataTypes.DoubleType</code></td>
</tr>
<tr>
<td style="text-align:left">DecimalType</td>
<td style="text-align:left">java.math.BigDecimal</td>
<td style="text-align:left"><code>DataTypes.createDecimalType()</code></td>
</tr>
<tr>
<td style="text-align:left">StringType</td>
<td style="text-align:left">String</td>
<td style="text-align:left"><code>DataTypes.StringType</code></td>
</tr>
<tr>
<td style="text-align:left">BinaryType</td>
<td style="text-align:left">byte[]</td>
<td style="text-align:left"><code>DataTypes.BinaryType</code></td>
</tr>
<tr>
<td style="text-align:left">BooleanType</td>
<td style="text-align:left">boolean或Boolean</td>
<td style="text-align:left"><code>DataTypes.BooleanType</code></td>
</tr>
<tr>
<td style="text-align:left">TimestampType</td>
<td style="text-align:left">java.sql.TimeStamp</td>
<td style="text-align:left"><code>DataTypes.TimestampType</code></td>
</tr>
<tr>
<td style="text-align:left">DateType</td>
<td style="text-align:left">java.sql.Date</td>
<td style="text-align:left"><code>DataTypes.DateType</code></td>
</tr>
<tr>
<td style="text-align:left">ArrayType</td>
<td style="text-align:left">java.util.List</td>
<td style="text-align:left"><code>DataTypes.createArrayType(elementType [, containsNull])</code><br>containsNull的默认值为True</td>
</tr>
<tr>
<td style="text-align:left">MapType</td>
<td style="text-align:left">java.util.Map</td>
<td style="text-align:left"><code>DataTypes.createMapType(keyType, valueType, [valueContainsNull])</code><br>valueContainsNull的默认值为True</td>
</tr>
<tr>
<td style="text-align:left">StructType</td>
<td style="text-align:left">org.apache.spark.sql.Row</td>
<td style="text-align:left"><code>DataTypes.createStructType(fields)</code><br><code>field</code>是一个包含多个StructFile的Array，并且任意两个StructField不能同名</td>
</tr>
<tr>
<td style="text-align:left">StructField</td>
<td style="text-align:left">该字段对应Scala数据类型<br>eg：int是IntegerType的StructField</td>
<td style="text-align:left"><code>DataTypes.createStructField(name, dataType, [nullable])</code><br><code>nullable</code>指定该field是否可以为空值，默认值为True</td>
</tr>
</tbody>
</table>
<h2 id="Python类型"><a class="header-anchor" href="#Python类型"></a>Python类型</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 使用正确的Python类型</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line">b = ByteType()</span><br></pre></td></tr></table></figure>
<p>Python类型参考表：</p>
<table>
<thead>
<tr>
<th style="text-align:left">数据类型</th>
<th style="text-align:left">值类型</th>
<th style="text-align:left">获取或创建数据类型的API</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">ByteType</td>
<td style="text-align:left">int或long<br>1. 数字在运行时转换为1字节的带符号整数<br>2. 数字范围为-128<sub>127，即$-2^7</sub>2^7-1$</td>
<td style="text-align:left"><code>ByteType()</code></td>
</tr>
<tr>
<td style="text-align:left">ShortType</td>
<td style="text-align:left">int或long<br>1. 数字在运行时将转换为2字节带符号的整数<br>2. 数字范围为-32768<sub>32767，即$-2^{15}</sub>2^{15}-1$</td>
<td style="text-align:left"><code>ShortType()</code></td>
</tr>
<tr>
<td style="text-align:left">IntegerType</td>
<td style="text-align:left">int或long<br>若使用IntegerType()，则太大的数字将被Spark SQL拒绝，则最好使用LongType()</td>
<td style="text-align:left"><code>IntegerType()</code></td>
</tr>
<tr>
<td style="text-align:left">LongType</td>
<td style="text-align:left">long<br>1. 数字在运行时将转换为8字节带符号整数<br>2. 数字范围为$-9223372036854775808<sub>9223372036854775807$，即$-2^{63}</sub>2^{63}-1$</td>
<td style="text-align:left"><code>LongType()</code></td>
</tr>
<tr>
<td style="text-align:left">FloatType</td>
<td style="text-align:left">float<br>数字在运行时将转换为4字节的单精度浮点数</td>
<td style="text-align:left"><code>FloatType()</code></td>
</tr>
<tr>
<td style="text-align:left">DoubleType</td>
<td style="text-align:left">float</td>
<td style="text-align:left"><code>DoubleType()</code></td>
</tr>
<tr>
<td style="text-align:left">DecimalType</td>
<td style="text-align:left">decimal.Decimal</td>
<td style="text-align:left"><code>DecimalType()</code></td>
</tr>
<tr>
<td style="text-align:left">StringType</td>
<td style="text-align:left">string</td>
<td style="text-align:left"><code>StringType()</code></td>
</tr>
<tr>
<td style="text-align:left">BinaryType</td>
<td style="text-align:left">bytearray</td>
<td style="text-align:left"><code>BinaryType()</code></td>
</tr>
<tr>
<td style="text-align:left">BooleanType</td>
<td style="text-align:left">bool</td>
<td style="text-align:left"><code>BooleanType()</code></td>
</tr>
<tr>
<td style="text-align:left">TimestampType</td>
<td style="text-align:left">datetime.datetime</td>
<td style="text-align:left"><code>TimestampType()</code></td>
</tr>
<tr>
<td style="text-align:left">DateType</td>
<td style="text-align:left">datetime.date</td>
<td style="text-align:left"><code>DateType()</code></td>
</tr>
<tr>
<td style="text-align:left">ArrayType</td>
<td style="text-align:left">list, tuple, array</td>
<td style="text-align:left"><code>ArrayType(elementType, [containsNull])</code><br>containsNull的默认值为True</td>
</tr>
<tr>
<td style="text-align:left">MapType</td>
<td style="text-align:left">字典</td>
<td style="text-align:left"><code>MapType(keyType, valueType, [valueContainsNull])</code><br>valueContainsNull的默认值为True</td>
</tr>
<tr>
<td style="text-align:left">StructType</td>
<td style="text-align:left">列表或元组</td>
<td style="text-align:left"><code>StructType(fields)</code><br><code>field</code>是一个包含多个StructFile的list，并且任意两个StructField不能同名</td>
</tr>
<tr>
<td style="text-align:left">StructField</td>
<td style="text-align:left">该字段对应Python数据类型<br>eg：int是IntegerType的StructField</td>
<td style="text-align:left"><code>StructField(name, dataType, [nullable])</code><br><code>nullable</code>指定该field是否可以为空值，默认值为True</td>
</tr>
</tbody>
</table>
<h2 id="Scala类型"><a class="header-anchor" href="#Scala类型"></a>Scala类型</h2>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 可使用正确的Scala类型</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._val</span><br><span class="line">b = <span class="type">ByteType</span></span><br></pre></td></tr></table></figure>
<p>Scala类型参考表：</p>
<table>
<thead>
<tr>
<th style="text-align:left">数据类型</th>
<th style="text-align:left">值类型</th>
<th style="text-align:left">获取或创建数据类型的API</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">ByteType</td>
<td style="text-align:left">Byte</td>
<td style="text-align:left"><code>ByteType</code></td>
</tr>
<tr>
<td style="text-align:left">ShortType</td>
<td style="text-align:left">Short</td>
<td style="text-align:left"><code>ShortType</code></td>
</tr>
<tr>
<td style="text-align:left">IntegerType</td>
<td style="text-align:left">Int</td>
<td style="text-align:left"><code>IntegerType</code></td>
</tr>
<tr>
<td style="text-align:left">LongType</td>
<td style="text-align:left">Long</td>
<td style="text-align:left"><code>LongType</code></td>
</tr>
<tr>
<td style="text-align:left">FloatType</td>
<td style="text-align:left">Float</td>
<td style="text-align:left"><code>FloatType</code></td>
</tr>
<tr>
<td style="text-align:left">DoubleType</td>
<td style="text-align:left">Double</td>
<td style="text-align:left"><code>DoubleType</code></td>
</tr>
<tr>
<td style="text-align:left">DecimalType</td>
<td style="text-align:left">java.math.BigDecimal</td>
<td style="text-align:left"><code>DecimalType</code></td>
</tr>
<tr>
<td style="text-align:left">StringType</td>
<td style="text-align:left">String</td>
<td style="text-align:left"><code>StringType</code></td>
</tr>
<tr>
<td style="text-align:left">BinaryType</td>
<td style="text-align:left">Array[Type]</td>
<td style="text-align:left"><code>BinaryType</code></td>
</tr>
<tr>
<td style="text-align:left">BooleanType</td>
<td style="text-align:left">Boolean</td>
<td style="text-align:left"><code>BooleanType</code></td>
</tr>
<tr>
<td style="text-align:left">TimestampType</td>
<td style="text-align:left">java.sql.TimeStamp</td>
<td style="text-align:left"><code>TimestampType</code></td>
</tr>
<tr>
<td style="text-align:left">DateType</td>
<td style="text-align:left">java.sql.Date</td>
<td style="text-align:left"><code>DateType</code></td>
</tr>
<tr>
<td style="text-align:left">ArrayType</td>
<td style="text-align:left">scala.collection.Seq</td>
<td style="text-align:left"><code>ArrayType(elementType, [containsNull])</code><br>containsNull的默认值为True</td>
</tr>
<tr>
<td style="text-align:left">MapType</td>
<td style="text-align:left">scala.collection.Map</td>
<td style="text-align:left"><code>MapType(keyType, valueType, [valueContainsNull])</code><br>valueContainsNull的默认值为True</td>
</tr>
<tr>
<td style="text-align:left">StructType</td>
<td style="text-align:left">org.apache.spark.sql.Row</td>
<td style="text-align:left"><code>StructType(fields)</code><br><code>field</code>是一个包含多个StructFile的Array，并且任意两个StructField不能同名</td>
</tr>
<tr>
<td style="text-align:left">StructField</td>
<td style="text-align:left">该字段对应Scala数据类型<br>eg：int是IntegerType的StructField</td>
<td style="text-align:left"><code>StructField(name, dataType, [nullable])</code><br><code>nullable</code>指定该field是否可以为空值，默认值为True</td>
</tr>
</tbody>
</table>
<h1 id="DataFrame-2"><a class="header-anchor" href="#DataFrame-2"></a>DataFrame</h1>
<p>DataFrame由记录（record）组成</p>
<ul>
<li>record是Row类型</li>
<li>一条record由多列组成</li>
<li><strong>分区</strong>定义了DataFrame以及Dataset在集群上的物理分布</li>
<li><strong>划分模式</strong>定义了partition的分配方式</li>
<li>当使用DataFrame时，向驱动器请求行的命令总是返回一个或多个Row类型的行数据</li>
</ul>
<h2 id="模式"><a class="header-anchor" href="#模式"></a>模式</h2>
<p>模式（schema）定义了DataFrame列的名称以及列的数据类型，可以由数据源来定义模式（即读时模式，schema-on-read），也可以由我们自己来显式定义。</p>
<p>实际应用场景决定了定义Schema的方式：</p>
<ul>
<li>当应用于即席（Ad-hoc）分析时，使用读时模式
<ul>
<li>在处理如csv和json等纯文本（无类型）时速度较慢</li>
</ul>
</li>
<li>当使用Spark进行生产级别ETL（Extract提取、Transform转换、Load加载）时，使用显示定义</li>
</ul>
<p>一个模式是由许多字段（StructField）构成的StructType；这些字段是StructField，具有名称、类型、布尔标志（指定该列是否可以包含缺失值或空值），且用户可指定与该列关联的元数据（metadata）。</p>
<ul>
<li>如果在运行时，数据的类型与定义的Schema模式不匹配，Spark将抛出一个错误</li>
<li>通过<code>printSchema()</code>函数查询DataFrame的模式</li>
<li>只有DataFrame具有模式，行对象本身没有模式</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line"><span class="comment">// 创建DataFrame并指定模式</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StructField</span>, <span class="type">StructType</span>, <span class="type">StringType</span>, <span class="type">LongType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.<span class="type">Metadata</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> myManualSchema = <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">   <span class="type">StructField</span>(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">   <span class="type">StructField</span>(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">   <span class="type">StructField</span>(<span class="string">&quot;count&quot;</span>, <span class="type">LongType</span>, <span class="literal">false</span>, </span><br><span class="line">   <span class="type">Metadata</span>.fromJason(<span class="string">&quot;&#123;\&quot;hello\&quot;:\&quot;world\&quot;&#125;&quot;</span>))</span><br><span class="line">))</span><br><span class="line"><span class="keyword">val</span> df = spark.read.format(<span class="string">&quot;json&quot;</span>).schema(myManualSchema)</span><br><span class="line">   .load(<span class="string">&quot;/data/flight-data/json/2015-summary.json&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in python</span></span><br><span class="line"><span class="comment"># 创建DataFrame并指定模式</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructField, StructType, StringType, LongType</span><br><span class="line"></span><br><span class="line">myManualSchema = StructType([</span><br><span class="line">   StructField(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">   StructField(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">   StructField(<span class="string">&quot;count&quot;</span>, LongType(), <span class="literal">False</span>, metadata=&#123;<span class="string">&quot;hello&quot;</span>:<span class="string">&quot;world&quot;</span>&#125;)</span><br><span class="line">])</span><br><span class="line">df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;json&quot;</span>).schema(myManualSchema)\</span><br><span class="line">   .load(<span class="string">&quot;/data/flight-data/json/2015-summary.json&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="列"><a class="header-anchor" href="#列"></a>列</h2>
<p>Spark中的列与电子表格、R dataframe、pandas DataFrame中的列类似，可以对DataFrame中的列进行选择、转换操作和删除，并将这些操作表示为<strong>表达式</strong>。</p>
<p>对Spark来说，列是逻辑结构，只是表示根据表达式为每个记录计算出的值。</p>
<ul>
<li>可通过<code>col</code>函数、<code>column</code>函数构造和引用列</li>
<li>列和数据表的解析在分析器阶段发生</li>
<li>Scala还可使用下列表达式创建列
<ul>
<li>符号<code>$</code>将字符串指定为表达式</li>
<li>符号<code>'</code>指定一个symbol，是Scala引用标识符的特殊结构</li>
</ul>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$<span class="string">&quot;myColumn&quot;</span></span><br><span class="line">&#x27;myColumn</span><br></pre></td></tr></table></figure>
<ul>
<li>显示应用列：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引用某DataFrame的某一列</span></span><br><span class="line"> df.col(<span class="string">&quot;columnName&quot;</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="行"><a class="header-anchor" href="#行"></a>行</h2>
<ul>
<li>只有DataFrame具有模式，行（Row）对象本身没有模式</li>
<li>手动创建Row对象，必须按照该行所属的DataFrame的列顺序来初始化Row对象</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line"><span class="comment">// 创建Row对象</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">val</span> myRow = <span class="type">Row</span>(<span class="string">&quot;Hello&quot;</span>, <span class="literal">null</span>, <span class="number">1</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in python</span></span><br><span class="line"><span class="comment"># 创建Row对象</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line">myRow = Row(<span class="string">&quot;Hello&quot;</span>, <span class="literal">None</span>, <span class="number">1</span>, <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>使用Scala或Java访问行时，需要使用辅助方法或显式地指定值类型</li>
<li>使用Python或R访问行时，访问行时，值被自动转化为正确的类型</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line"><span class="comment">// 访问Row对象</span></span><br><span class="line">myRow(<span class="number">0</span>)  <span class="comment">// 任意类型</span></span><br><span class="line">myRow(<span class="number">0</span>).asInstanceOf[<span class="type">String</span>]  <span class="comment">// 字符串</span></span><br><span class="line">myRow.getString(<span class="number">0</span>)  <span class="comment">// 字符串</span></span><br><span class="line">myRow.getInt(<span class="number">2</span>)  <span class="comment">// 整型</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 访问Row对象</span></span><br><span class="line">myRow[<span class="number">0</span>]</span><br><span class="line">myRow[<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<h2 id="表达式"><a class="header-anchor" href="#表达式"></a>表达式</h2>
<p>表达式（expression）是对一个DataFrame中某一个记录的一个或多个值的一组转换操作。</p>
<ul>
<li>最简单的表达式：通过<code>expr</code>函数创建的表达式，仅仅是一个DataFrame列的引用。即，<code>expr(&quot;someCol&quot;)</code>等同于<code>col(someCol)</code>。</li>
<li><code>expr</code>函数可以将字符串解析成转换操作和列引用，也可以在之后将其传递到下一步的转换操作</li>
</ul>
<blockquote>
<p><code>expr(&quot;someCol - 5&quot;)</code>与<code>col(&quot;someCol&quot;) - 5</code>、<code>expr(&quot;someCol&quot;) - 5</code>，都是相同的转换操作，Spark将它们编译为表示操作顺序的逻辑树</p>
</blockquote>
<ol>
<li>列只是表达式</li>
<li>列与对这些列的转换操作被编译后生成的逻辑计划，与解析后的表达式的逻辑计划是一样的</li>
</ol>
<h2 id="转换操作-2"><a class="header-anchor" href="#转换操作-2"></a>转换操作</h2>
<p>图<br>
70页<br>
待补充</p>
<ul>
<li>创建DataFrame：<code>createDataFrame()</code></li>
<li>在Scala中，可以利用Spark的隐式方法（使用<code>implicit</code>关键字），对Seq类型执行<code>toDF</code>函数来实现
<ul>
<li>但是该方法对<code>null</code>类型的支持并不稳定，并不推荐在实际生产中使用</li>
</ul>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> myDF = <span class="type">Seq</span>((<span class="string">&quot;Hello&quot;</span>, <span class="number">2</span>, <span class="number">1</span>L)).toDF(<span class="string">&quot;col1&quot;</span>, <span class="string">&quot;col2&quot;</span>, <span class="string">&quot;col3&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>select</code>、<code>selectExpr</code>支持在DataFrame上执行类似数据表的SQL查询</li>
<li><code>select</code>：处理列或表达式</li>
<li><code>selectExpr</code>：处理字符串表达式</li>
</ul>
<h1 id="函数"><a class="header-anchor" href="#函数"></a>函数</h1>
<h2 id="apply"><a class="header-anchor" href="#apply"></a>apply()</h2>
<p>获取指定字段</p>
<ul>
<li>返回对象为Column类型</li>
<li>只能获取一个字段</li>
<li>而<code>col</code>、<code>column</code>可以获取多个指定字段</li>
</ul>
<h2 id="cast"><a class="header-anchor" href="#cast"></a>cast()</h2>
<p>用于更改列的类型（来转换数据类型）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.withColumn(<span class="string">&quot;count&quot;</span>, col(<span class="string">&quot;count&quot;</span>).cast(<span class="string">&quot;long&quot;</span>))</span><br></pre></td></tr></table></figure>
<p>等价于</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span>, <span class="built_in">CAST</span>(count <span class="keyword">AS</span> LONG) <span class="keyword">AS</span> count2 <span class="keyword">FROM</span> dfTable;</span><br></pre></td></tr></table></figure>
<h2 id="coalesce"><a class="header-anchor" href="#coalesce"></a>coalesce()</h2>
<p>合并操作</p>
<ul>
<li>不会导致数据的全面洗牌，但是会尝试合并分区</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in scala or Python</span></span><br><span class="line">df.repartition(<span class="number">5</span>, col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)).coalesce(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="collectAsList"><a class="header-anchor" href="#collectAsList"></a>collectAsList()</h2>
<p>获取所有数据到List</p>
<h2 id="columns"><a class="header-anchor" href="#columns"></a>columns</h2>
<p>使用属性<code>columns</code>查询DataFrame的所有列</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line">spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/data/flight-data/json/2015-summary.json&quot;</span>).columns</span><br></pre></td></tr></table></figure>
<h2 id="contains"><a class="header-anchor" href="#contains"></a>contains()</h2>
<p>检查在某列上是否存在某字符串</p>
<ul>
<li>Scala函数</li>
<li>返回Boolean值</li>
<li>在Python和SQL中，应使用<code>instr</code>函数</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">val</span> containsBlack = col(<span class="string">&quot;Description&quot;</span>).contains(<span class="string">&quot;BLACK&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> containsWhite = col(<span class="string">&quot;DESCRIPTION&quot;</span>).contains(<span class="string">&quot;WHITE&quot;</span>)</span><br><span class="line">df.withColumn(<span class="string">&quot;hasSimpleColor&quot;</span>, containsBlack.or(containsWhite))</span><br><span class="line">  .where(<span class="string">&quot;hasSimpleColor&quot;</span>)</span><br><span class="line">  .select(<span class="string">&quot;Description&quot;</span>).show(<span class="number">3</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
<h2 id="createDataFrame"><a class="header-anchor" href="#createDataFrame"></a>createDataFrame()</h2>
<p>创建DataFrame</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StructField</span>, <span class="type">StructType</span>, <span class="type">StringType</span>, <span class="type">LongType</span>&#125;</span><br><span class="line"><span class="keyword">val</span> myManualSchema = <span class="keyword">new</span> <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">   <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;some&quot;</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">   <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;col&quot;</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">   <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;names&quot;</span>, <span class="type">LongType</span>, <span class="literal">false</span>)))</span><br><span class="line"><span class="keyword">val</span> myRows = <span class="type">Seq</span>(<span class="type">Row</span>(<span class="string">&quot;Hello&quot;</span>, <span class="literal">null</span>, <span class="number">1</span>L))</span><br><span class="line"><span class="keyword">val</span> myRDD = spark.sparkContext.parallelize(myRows)</span><br><span class="line"><span class="keyword">val</span> myDf = spark.createDataFrame(myRDD, myManualSchema)</span><br><span class="line">myDf.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructField, StructType, StringType, LongType</span><br><span class="line"></span><br><span class="line">myManualSchema = StructType([</span><br><span class="line">   StructField(<span class="string">&quot;some&quot;</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">   StructField(<span class="string">&quot;col&quot;</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">   StructField(<span class="string">&quot;names&quot;</span>, LongType(), <span class="literal">False</span>)</span><br><span class="line">])</span><br><span class="line">myRow = Row(<span class="string">&quot;Hello&quot;</span>, <span class="literal">None</span>, <span class="number">1</span>)</span><br><span class="line">myDf = spark.createDataFrame([myRow], myManualSchema)</span><br><span class="line">myDf.show()</span><br></pre></td></tr></table></figure>
<h2 id="createOrReplaceTempView"><a class="header-anchor" href="#createOrReplaceTempView"></a>createOrReplaceTempView()</h2>
<p>创建临时视图，便于用SQL访问</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">val</span> df = spark.read.format(<span class="string">&quot;json&quot;</span>)</span><br><span class="line">   .load(<span class="string">&quot;/data/flight-data/json/2015-summary.json&quot;</span>)</span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;dfTable&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/data/flight-data/json/2015-summary.json&quot;</span>)</span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;dfTable&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="def"><a class="header-anchor" href="#def"></a>def</h2>
<p>自定义函数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">val</span> udfExampleDF = spark.range(<span class="number">5</span>).toDF(<span class="string">&quot;num&quot;</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">power3</span></span>(number:<span class="type">Double</span>):</span><br><span class="line">    <span class="type">Double</span> = number * number * number</span><br><span class="line">power3(<span class="number">2.0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">udfExampleDF = spark.<span class="built_in">range</span>(<span class="number">5</span>).toDF(<span class="string">&quot;num&quot;</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">power3</span>(<span class="params">double_value</span>):</span><br><span class="line">    <span class="keyword">return</span> double_value ** <span class="number">3</span></span><br><span class="line">power3(<span class="number">2.0</span>)</span><br></pre></td></tr></table></figure>
<p>目前到115页</p>
<h2 id="distinct"><a class="header-anchor" href="#distinct"></a>distinct()</h2>
<p>去重</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">df.select(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>, <span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).distinct().count()</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span>(ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME)) <span class="keyword">FROM</span> dfTable;</span><br></pre></td></tr></table></figure>
<h2 id="drop"><a class="header-anchor" href="#drop"></a>drop()</h2>
<p>删除列；可以通过传入多个列作为参数来同事删除多个列</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.drop(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="eqNullSafe"><a class="header-anchor" href="#eqNullSafe"></a>eqNullSafe()</h2>
<h2 id="euqalTo"><a class="header-anchor" href="#euqalTo"></a>euqalTo()</h2>
<p>Scala中的“等于”，等价于<code>===</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.col</span><br><span class="line">df.where(col(<span class="string">&quot;InvoiceNo&quot;</span>).equalTo(<span class="number">536365</span>))</span><br><span class="line">   .select(<span class="string">&quot;InvoiceNo&quot;</span>, <span class="string">&quot;Description&quot;</span>)</span><br><span class="line">   .show(<span class="number">5</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="comment">// 等价于</span></span><br><span class="line">df.where(col(<span class="string">&quot;InvoiceNo&quot;</span>) === <span class="number">536365</span>)</span><br><span class="line">   .select(<span class="string">&quot;InvoiceNo&quot;</span>, <span class="string">&quot;Description&quot;</span>)</span><br><span class="line">   .show(<span class="number">5</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
<h2 id="explain"><a class="header-anchor" href="#explain"></a>explain()</h2>
<ul>
<li>可以通过<code>explain</code>函数观察到Spark是如何执行查询操作的</li>
<li>调用某个DataFrame的<code>explain</code>操作会显示DataFrame的来源</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># in scala</span><br><span class="line">flightData.sort(<span class="string">&quot;count&quot;</span>).explain()</span><br><span class="line">==<span class="type">Physical</span> <span class="type">Plan</span>==</span><br><span class="line">*<span class="type">Sort</span> [count#<span class="number">195</span> <span class="type">ASC</span> <span class="type">NULLS</span> <span class="type">FIRST</span>], <span class="literal">true</span>, <span class="number">0</span></span><br><span class="line">+- <span class="type">Exchange</span> rangepartitioning(count#<span class="number">195</span> <span class="type">ASC</span> <span class="type">NULLS</span> <span class="type">FIRST</span>, <span class="number">200</span>)</span><br><span class="line">   +- <span class="type">FileScan</span> csv [<span class="type">DEST_COUNTRY_NAME</span>#<span class="number">193</span>, <span class="type">ORIGIN_COUNTRY_NAME</span>#<span class="number">194</span>, count#<span class="number">195</span>] ...</span><br></pre></td></tr></table></figure>
<h2 id="first"><a class="header-anchor" href="#first"></a>first()</h2>
<p>在DataFrame上调用<code>first()</code>查看一行（获取第一行记录）</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.<span class="keyword">first</span>()</span><br></pre></td></tr></table></figure>
<h2 id="groupBy"><a class="header-anchor" href="#groupBy"></a>groupBy()</h2>
<h2 id="head"><a class="header-anchor" href="#head"></a>head()</h2>
<ul>
<li><code>head</code>：获取第一行记录</li>
<li><code>head(n: Int)</code>：获取前n行记录</li>
</ul>
<h2 id="leq"><a class="header-anchor" href="#leq"></a>leq()</h2>
<p>小于等于</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;expr, not, col&#125;</span><br><span class="line">df.withColumn(<span class="string">&quot;isExpensive&quot;</span>, not(col(<span class="string">&quot;UnitPrice&quot;</span>).leq(<span class="number">250</span>)))</span><br><span class="line">  .filter(<span class="string">&quot;isExpensive&quot;</span>)</span><br><span class="line">  .select(<span class="string">&quot;Description&quot;</span>, <span class="string">&quot;UnitPrice&quot;</span>).show(<span class="number">5</span>)</span><br><span class="line">df.withColumn(<span class="string">&quot;isExpensive&quot;</span>, expr(<span class="string">&quot;NOT UnitPrice &lt;= 250&quot;</span>))</span><br><span class="line">  .filter(<span class="string">&quot;isExpensive&quot;</span>)</span><br><span class="line">  .select(<span class="string">&quot;Description&quot;</span>, <span class="string">&quot;UnitPrice&quot;</span>).show(<span class="number">5</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="limit"><a class="header-anchor" href="#limit"></a>limit()</h2>
<p>限制提取的记录数目</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala or Python</span></span><br><span class="line">df.limit(<span class="number">5</span>).show()</span><br><span class="line">df.orderBy(expr(<span class="string">&quot;count desc&quot;</span>)).limit(<span class="number">6</span>).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable LIMIT <span class="number">6</span>;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable <span class="keyword">ORDER</span> <span class="keyword">BY</span> count <span class="keyword">desc</span> LIMIT <span class="number">6</span>;</span><br></pre></td></tr></table></figure>
<h2 id="na"><a class="header-anchor" href="#na"></a>na</h2>
<h3 id="drop-2"><a class="header-anchor" href="#drop-2"></a>drop()</h3>
<p>删除包含NULL的行</p>
<ul>
<li>参数<code>any</code>：当存在一个值是<code>NULL</code>时，就删除该行</li>
<li>参数<code>all</code>：当所有的值为<code>NULL</code>或<code>NaN</code>时，才删除该行</li>
<li>也可指定某几列，对这些列进行删除空值操作</li>
</ul>
<figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.na.<span class="built_in">drop</span>()</span><br><span class="line">df.na.<span class="built_in">drop</span>(<span class="string">&quot;any&quot;</span>)</span><br><span class="line">df.na.<span class="built_in">drop</span>(<span class="string">&quot;all&quot;</span>)</span><br><span class="line">df.na.<span class="built_in">drop</span>(<span class="string">&quot;all&quot;</span>, Seq(<span class="string">&quot;StockCode&quot;</span>, <span class="string">&quot;InvoiceNo&quot;</span>))  <span class="comment">// in Scala</span></span><br><span class="line">df.na.<span class="built_in">drop</span>(<span class="string">&quot;all&quot;</span>, subset = [<span class="string">&quot;StockCode&quot;</span>, <span class="string">&quot;InvoiceNo&quot;</span>])  <span class="meta"># in Python</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 在SQL中需要逐列删除包含NULL的行</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable <span class="keyword">WHERE</span> Description <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>;</span><br></pre></td></tr></table></figure>
<h3 id="fill"><a class="header-anchor" href="#fill"></a>fill()</h3>
<p>可以通过指定一个映射（用一个特定值和一组列），用一组值填充一列或多列</p>
<ul>
<li>可以使用Scala的Map映射实现针对不同的列指定不同的映射方案</li>
</ul>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df<span class="selector-class">.na</span><span class="selector-class">.fill</span>(<span class="string">&quot;All Null values become this string&quot;</span>) <span class="comment">// 用字符串替换列中的所有NULL值</span></span><br><span class="line">df<span class="selector-class">.na</span><span class="selector-class">.fill</span>(<span class="number">5</span>:Integer)  <span class="comment">// 用5填充Integer类型的列中的NULL值</span></span><br><span class="line">df<span class="selector-class">.na</span><span class="selector-class">.fill</span>(<span class="number">5</span>:Double)  <span class="comment">// 用5填充Double类型的列中的NULL值</span></span><br><span class="line">df<span class="selector-class">.na</span><span class="selector-class">.fill</span>(<span class="number">5</span>， <span class="built_in">Seq</span>(<span class="string">&quot;StockCode&quot;</span>， <span class="string">&quot;InvoiceNo&quot;</span>))  <span class="comment">// 指定多列</span></span><br><span class="line"></span><br><span class="line">val fillColValues = <span class="built_in">Map</span>(<span class="string">&quot;StockCode&quot;</span> -&gt; <span class="number">5</span> ， <span class="string">&quot;Description&quot;</span> -&gt; <span class="string">&quot;No Value&quot;</span>)  <span class="comment">// 指定不同的替换值</span></span><br><span class="line">df<span class="selector-class">.na</span><span class="selector-class">.fill</span>(fillColValues)</span><br></pre></td></tr></table></figure>
<h3 id="replace"><a class="header-anchor" href="#replace"></a>replace()</h3>
<p>根据当前值，替换掉某列中的所有值</p>
<ul>
<li>要求替换值与原始值的类型相同</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line">df.na.replace(<span class="string">&quot;Description&quot;</span>， <span class="type">Map</span>(<span class="string">&quot;&quot;</span> -&gt; <span class="string">&quot;UNKNOWN&quot;</span>))</span><br><span class="line"><span class="comment">// 将Description列中的空值替换为&quot;UNKNOWN&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">df.na.replace([<span class="string">&quot;&quot;</span>]， [<span class="string">&quot;UNKNOWN&quot;</span>]， <span class="string">&quot;Description&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="or"><a class="header-anchor" href="#or"></a>or()</h2>
<p>或</p>
<ul>
<li><code>or</code>语句需要在同一个语句中指定</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">val</span> priceFilter = col(<span class="string">&quot;UnitPrice&quot;</span>) &gt; <span class="number">600</span>  <span class="comment">// 筛选条件1</span></span><br><span class="line"><span class="keyword">val</span> descripFilter = col(<span class="string">&quot;Description&quot;</span>).contains(<span class="string">&quot;POSTAGE&quot;</span>)  <span class="comment">// 筛选条件2</span></span><br><span class="line">df.where(col(<span class="string">&quot;StockCode&quot;</span>).isin(<span class="string">&quot;DOT&quot;</span>)).where(priceFilter.or(descripFilter))</span><br><span class="line">   .show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> instr</span><br><span class="line">priceFilter = col(<span class="string">&quot;UnitPrice&quot;</span>) &gt; <span class="number">600</span></span><br><span class="line">descripFilter = instr(df.Description, <span class="string">&quot;POSTAGE&quot;</span>) &gt;= <span class="number">1</span></span><br><span class="line">df.where(df.StockCode.isin(<span class="string">&quot;DOT&quot;</span>)).where(priceFilter | descripFilter).show()  <span class="comment">## Python的“或”为“|”</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable <span class="keyword">WHERE</span> StockCode <span class="keyword">in</span> (&quot;DOT&quot;) <span class="keyword">AND</span>(UnitPrice <span class="operator">&gt;</span> <span class="number">600</span> <span class="keyword">OR</span> instr(Description, &quot;POSTAGE&quot;) <span class="operator">&gt;=</span> <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="orderBy"><a class="header-anchor" href="#orderBy"></a>orderBy()</h2>
<p>对DataFrame的值进行排序</p>
<ul>
<li><code>sort</code>和<code>orderBy</code>是等价的，均接收列表达式、字符串，以及多个列</li>
<li>默认按升序（asc）排序</li>
<li>如果要指定降序排序，则需使用<code>desc()</code>函数</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line">df.sort(<span class="string">&quot;count&quot;</span>).show(<span class="number">5</span>)</span><br><span class="line">df.orderBy(<span class="string">&quot;count&quot;</span>, <span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).show(<span class="number">5</span>)</span><br><span class="line">df.orderBy(col(<span class="string">&quot;count&quot;</span>), col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)).show(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;desc, asc&#125;</span><br><span class="line">df.orderBy(expr(<span class="string">&quot;count desc&quot;</span>)).show(<span class="number">2</span>)  <span class="comment">// 指定降序排序</span></span><br><span class="line">df.orderBy(desc(<span class="string">&quot;count&quot;</span>), asc(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)).show(<span class="number">2</span>)  <span class="comment">// 分别指定降序、升序排序</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">df.sort(<span class="string">&quot;count&quot;</span>).show(<span class="number">5</span>)</span><br><span class="line">df.orderBy(<span class="string">&quot;count&quot;</span>, <span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).show(<span class="number">5</span>)</span><br><span class="line">df.orderBy(col(<span class="string">&quot;count&quot;</span>), col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)).show(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> desc, asc</span><br><span class="line">df.orderBy(expr(<span class="string">&quot;count desc&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line">df.orderBy(col(<span class="string">&quot;count&quot;</span>).desc(), col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).asc()).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable <span class="keyword">ORDER</span> <span class="keyword">BY</span> count <span class="keyword">DESC</span>, DEST_COUNTRY_NAME <span class="keyword">ASC</span> LIMIT <span class="number">2</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>可以指定空值在排序列表中的位置
<ul>
<li><code>asc_nulls_first</code>指示空值排在升序排列之前</li>
<li><code>desc_nulls_first</code>指示空值排在降序排列之前</li>
<li><code>asc_nulls_last</code>指示空值排在升序排列后面</li>
</ul>
</li>
</ul>
<h2 id="printSchema"><a class="header-anchor" href="#printSchema"></a>printSchema()</h2>
<p>查询DataFrame的模式（schema）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line"><span class="keyword">val</span> df = spark.read.format(<span class="string">&quot;json&quot;</span>)</span><br><span class="line">   .load(<span class="string">&quot;/data/flight-data/json/2015-summary.json&quot;</span>)</span><br><span class="line"><span class="comment">// 查询DataFrame的模式</span></span><br><span class="line">df.printSchema()</span><br></pre></td></tr></table></figure>
<h2 id="randomSplit"><a class="header-anchor" href="#randomSplit"></a>randomSplit()</h2>
<p>随机分割</p>
<ul>
<li>需要设置分割比例</li>
<li>如果分割比例的和不为1，则比例参数会被归一化</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将DataFrame按25%和75%的比例分割</span></span><br><span class="line"><span class="keyword">val</span> dataFrames = df.randomSplit(<span class="type">Array</span>(<span class="number">0.25</span>, <span class="number">0.75</span>), <span class="number">8</span>)  <span class="comment">// 其中8为seed参数</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">dataFrames = df.randomSplit([<span class="number">0.25</span>, <span class="number">0.75</span>], <span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<h2 id="range"><a class="header-anchor" href="#range"></a>range()</h2>
<h2 id="read"><a class="header-anchor" href="#read"></a>read</h2>
<p>读取数据</p>
<ul>
<li>是一种转换操作</li>
<li>窄依赖</li>
<li>也是一种惰性操作——Spark并没有马上读取数据，直到在DataFrame上调用动作操作后才会真正读取数据</li>
</ul>
<p><code>option</code>参数：</p>
<ul>
<li><code>inferSchema=true</code>：模式推理，让Spark猜测DataFrame的模式</li>
<li><code>header=true</code>：指定文件的第一行为文件头</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line"><span class="keyword">val</span> flightData = spark</span><br><span class="line">   .read</span><br><span class="line">   .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">   .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">   .csv(<span class="string">&quot;/data/flight-data/csv/summary.csv&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in python</span></span><br><span class="line">flightData = spark\</span><br><span class="line">   .read\</span><br><span class="line">   .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)\</span><br><span class="line">   .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)\</span><br><span class="line">   .csv(<span class="string">&quot;/data/flight-data/csv/summary.csv&quot;</span>)</span><br><span class="line"><span class="comment"># 注意：python换行的话，行末需要加转义</span></span><br></pre></td></tr></table></figure>
<h2 id="repartition"><a class="header-anchor" href="#repartition"></a>repartition()</h2>
<p>重新分区</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala or Python</span></span><br><span class="line">df.repartition(<span class="number">5</span>)  <span class="comment">// 设置分区数为5</span></span><br><span class="line">df.repartition(col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>))  <span class="comment">// 按照某列DEST_COUNTRY_NAME进行分区</span></span><br><span class="line">df.repartition(<span class="number">5</span>, col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>))  <span class="comment">// 指定分区数和列</span></span><br></pre></td></tr></table></figure>
<h2 id="sample"><a class="header-anchor" href="#sample"></a>sample()</h2>
<p>随机抽样；按一定比例从DataFrame中随机抽取一部分行</p>
<ul>
<li>参数<code>withReplacement</code>指定是否放回抽样
<ul>
<li><code>true</code>为有放回抽样/有重复抽样</li>
<li><code>false</code>为无放回抽样/无重复抽样</li>
</ul>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line"><span class="keyword">val</span> seed = <span class="number">5</span></span><br><span class="line"><span class="keyword">val</span> withReplacement = <span class="literal">false</span> <span class="comment">// 无重复抽样</span></span><br><span class="line"><span class="keyword">val</span> fraction = <span class="number">0.5</span> <span class="comment">// 抽取50%</span></span><br><span class="line">df.sample(withReplacement, fraction, seed).count()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">seed = <span class="number">5</span></span><br><span class="line">withReplacement = <span class="literal">False</span></span><br><span class="line">fraction = <span class="number">0.5</span></span><br><span class="line">df.sample(withReplacement, fraction, seed).count()</span><br></pre></td></tr></table></figure>
<h2 id="select"><a class="header-anchor" href="#select"></a>select()</h2>
<p>处理列或表达式</p>
<ul>
<li>将待处理的列名字符串作为参数传递</li>
<li>添加多个列名字符串，可以选择多个列</li>
<li>不能混淆使用Column对象和字符串</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line">df.select(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// 选择多个列</span></span><br><span class="line">df.select(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>, <span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// 可以通过多种不同的方式引用列</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;expr, col, column&#125;</span><br><span class="line">df.select(</span><br><span class="line">   df.col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>),</span><br><span class="line">      col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>),</span><br><span class="line">      column(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>),</span><br><span class="line">      &#x27;<span class="type">DEST_COUNTRY_NAME</span>,</span><br><span class="line">      $<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>,</span><br><span class="line">      expr(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>))</span><br><span class="line">   .show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">df.select(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 选择多个列</span></span><br><span class="line">df.select(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>, <span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 可以通过多种不同的方式引用列</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> expr, col, column</span><br><span class="line">df.select(</span><br><span class="line">   expr(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>),</span><br><span class="line">   col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>),</span><br><span class="line">   column(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>))\</span><br><span class="line">.show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> DEST_COUNTRY_NAME <span class="keyword">FROM</span> dfTable LIMIT <span class="number">2</span></span><br><span class="line"><span class="comment">-- 选择多个列</span></span><br><span class="line"><span class="keyword">SELECT</span> DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME <span class="keyword">FROM</span> dfTable LIMIT <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h2 id="selectExpr"><a class="header-anchor" href="#selectExpr"></a>selectExpr()</h2>
<p>可以用于构造复杂表达式来创建DataFrame</p>
<ul>
<li>是常用的接口之一</li>
<li>可以添加任何不包含聚合操作的有效SQL语句</li>
<li>可以使用系统预定义的聚合函数来指定在整个DataFrame上的聚合操作</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line">df.selectExpr(</span><br><span class="line">   <span class="string">&quot;*&quot;</span>, <span class="comment">// 包含所有原始表中的列</span></span><br><span class="line">   <span class="string">&quot;(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry&quot;</span>  <span class="comment">// 增加新列withinCountry</span></span><br><span class="line">   )</span><br><span class="line">.show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// 使用系统预定义的聚合函数来指定在整个DataFrame上的聚合操作</span></span><br><span class="line">df.selectExpr(<span class="string">&quot;avg(count)&quot;</span>, <span class="string">&quot;count(distinct(DEST_COUNTRY_NAME))&quot;</span>).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">df.selectExpr(</span><br><span class="line">   <span class="string">&quot;*&quot;</span>, <span class="comment"># 包含所有原始表中的列</span></span><br><span class="line">   <span class="string">&quot;(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry&quot;</span>)\</span><br><span class="line">.show(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 使用系统预定义的聚合函数来指定在整个DataFrame上的聚合操作</span></span><br><span class="line">df.selectExpr(<span class="string">&quot;avg(count)&quot;</span>, <span class="string">&quot;count(distinct(DEST_COUNTRY_NAME))&quot;</span>).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span>, (DEST_COUNTRY_NAME <span class="operator">=</span> ORIGIN_COUNTRY_NAME) <span class="keyword">as</span> withinCountry</span><br><span class="line"><span class="keyword">FROM</span> dfTable</span><br><span class="line">LIMIT <span class="number">2</span>;</span><br><span class="line"><span class="comment">-- 使用系统预定义的聚合函数来指定在整个DataFrame上的聚合操作</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">AVG</span>(count), <span class="built_in">COUNT</span>(<span class="keyword">distinct</span>(DEST_COUNTRY_NAME)) <span class="keyword">FROM</span> dfTable LIMIT <span class="number">2</span>;</span><br></pre></td></tr></table></figure>
<h2 id="show"><a class="header-anchor" href="#show"></a>show()</h2>
<ul>
<li><code>show</code>：默认显示DataFrame的前20条记录</li>
<li><code>show(numRows: Int)</code>：显示numRows条记录</li>
<li><code>show(truncate: Boolean)</code>：是否每列最多只显示20个字符，默认为true <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.show(true)  <span class="comment"># 空格也算一个字符</span></span><br><span class="line">df.show(false)</span><br></pre></td></tr></table></figure>
</li>
<li><code>show(numRows: Int, truncate: Boolean)</code>：显示numRows条记录，且每列是否最多显示20个字符</li>
</ul>
<h2 id="sort"><a class="header-anchor" href="#sort"></a>sort()</h2>
<ul>
<li>宽依赖(因为行之间需要相互比较和交换)</li>
<li>不会修改DataFrame，通过转换DataFrame来返回新的DataFrame</li>
</ul>
<h2 id="sql"><a class="header-anchor" href="#sql"></a>sql()</h2>
<p>使用<code>spark.sql()</code>函数在SQL中查询数据，返回新的DataFrame</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line"><span class="keyword">val</span> sqlWay = spark.sql(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   SELECT DEST_COUNTRY_NAME, count(1)</span></span><br><span class="line"><span class="string">   FROM flight_data</span></span><br><span class="line"><span class="string">   GROUP BY DEST_COUNTRY_NAME</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> <span class="type">DataFrameWay</span> = flightData</span><br><span class="line">   .groupby(&#x27;<span class="type">DEST_COUNTRY_NAME</span>&#x27;)</span><br><span class="line">   .count()</span><br><span class="line"></span><br><span class="line">sqlWay.explain()</span><br><span class="line">== <span class="type">Physical</span> <span class="type">Plan</span> ==</span><br><span class="line">*<span class="type">HashAggregate</span>(keys=[<span class="type">DEST_COUNTRY_NAME</span>#<span class="number">182</span>], functions=[count(<span class="number">1</span>)])</span><br><span class="line">+- <span class="type">Exchange</span> hashpartitioning(<span class="type">DEST_COUNTRY_NAME</span>#<span class="number">182</span>, <span class="number">5</span>)</span><br><span class="line">   +- *<span class="type">HashAggregate</span>(keys=[<span class="type">DEST_COUNTRY_NAME</span>#<span class="number">182</span>], functions=[partial_count(<span class="number">1</span>)])</span><br><span class="line">      +- *<span class="type">FileScan</span> csv [<span class="type">DEST_COUNTRY_NAME</span>#<span class="number">182</span>] ...</span><br><span class="line"></span><br><span class="line"><span class="type">DataFrameWay</span>.explain()</span><br><span class="line">== <span class="type">Physical</span> <span class="type">Plan</span> ==</span><br><span class="line">*<span class="type">HashAggregate</span>(keys=[<span class="type">DEST_COUNTRY_NAME</span>#<span class="number">182</span>], functions=[count(<span class="number">1</span>)])</span><br><span class="line">+- <span class="type">Exchange</span> hashpartitioning(<span class="type">DEST_COUNTRY_NAME</span>#<span class="number">182</span>, <span class="number">5</span>)</span><br><span class="line">   +- *<span class="type">HashAggregate</span>(keys=[<span class="type">DEST_COUNTRY_NAME</span>#<span class="number">182</span>], functions=[partial_count(<span class="number">1</span>)])</span><br><span class="line">      +- *<span class="type">FileScan</span> csv [<span class="type">DEST_COUNTRY_NAME</span>#<span class="number">182</span>] ...</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line"><span class="keyword">val</span> maxSql = spark.sql(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   SELECT DEST_COUNTRY_NAME, sum(count) AS destination_total</span></span><br><span class="line"><span class="string">   FROM flight_data</span></span><br><span class="line"><span class="string">   GOURP BY DEST_COUNTRY_NAME</span></span><br><span class="line"><span class="string">   ORDER BY sum(count) DESC</span></span><br><span class="line"><span class="string">   LIMIT 5</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>)</span><br><span class="line">maxSql.show()</span><br></pre></td></tr></table></figure>
<h2 id="take"><a class="header-anchor" href="#take"></a>take()</h2>
<p><code>take(n: Int)</code>：获取前n行数据</p>
<ul>
<li><code>take</code>和<code>takeAsList</code>会将获得的数据返回到Driver端，使用时应注意数据量，以免Driver发生<code>OutOfMemoryError</code></li>
</ul>
<h2 id="takeAsList"><a class="header-anchor" href="#takeAsList"></a>takeAsList()</h2>
<p>获取前n行数据，并以List形式展现</p>
<ul>
<li><code>take</code>和<code>takeAsList</code>会将获得的数据返回到Driver端，使用时应注意数据量，以免Driver发生<code>OutOfMemoryError</code></li>
</ul>
<h2 id="toDF"><a class="header-anchor" href="#toDF"></a>toDF()</h2>
<h2 id="toLocalIterator"><a class="header-anchor" href="#toLocalIterator"></a>toLocalIterator()</h2>
<p>该函数是一个迭代器，将每个分区的数据返回给驱动器</p>
<ul>
<li>允许以串行的方式一个一个分区地迭代整个数据集</li>
<li>使用该函数，且分区很大时，很容易使驱动器节点崩溃并丢失应用程序的状态，代价很大</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">collectDF = df.limit(<span class="number">10</span>)</span><br><span class="line">collectDF.toLocalIterator()</span><br></pre></td></tr></table></figure>
<h2 id="union"><a class="header-anchor" href="#union"></a>union()</h2>
<p>连接/拼接两个DataFrame</p>
<ul>
<li>被连接的两个DataFrame需要具有完全相同的模式和列数</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">val</span> schema = df.schema</span><br><span class="line"><span class="keyword">val</span> newRows = <span class="type">Seq</span>(</span><br><span class="line">   <span class="type">Row</span>(<span class="string">&quot;New Country&quot;</span>, <span class="string">&quot;Other Country&quot;</span>, <span class="number">5</span>L),</span><br><span class="line">   <span class="type">Row</span>(<span class="string">&quot;New Country 2&quot;</span>, <span class="string">&quot;Other Country 3&quot;</span>, <span class="number">1</span>L)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> parallelizedRows = spark.sparkContext.parallelize(newRows)</span><br><span class="line"><span class="keyword">val</span> newDF = spark.createDataFrame(parallelizedRows, schema)</span><br><span class="line">df.union(newDF)</span><br><span class="line">   .where(<span class="string">&quot;count = 1&quot;</span>)</span><br><span class="line">   .where($<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span> =!= <span class="string">&quot;United States&quot;</span>)</span><br><span class="line">   .show()</span><br></pre></td></tr></table></figure>
<h2 id="where"><a class="header-anchor" href="#where"></a>where()</h2>
<p>过滤行，与<code>filter()</code>等价</p>
<ul>
<li>Spark同时执行所有过滤操作（不论过滤条件的先后顺序）</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.filter(col(<span class="string">&quot;count&quot;</span>) &lt; <span class="number">2</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// 在Scala和Python中等价于</span></span><br><span class="line">df.where(<span class="string">&quot;count &lt; 2&quot;</span>).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>等价于</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable <span class="keyword">WHERE</span> count <span class="operator">&lt;</span> <span class="number">2</span> LIMIT <span class="number">2</span>;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line">df.where(col(<span class="string">&quot;count&quot;</span>) &lt; <span class="number">2</span>).where(col(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>) =!= <span class="string">&quot;Croatia&quot;</span>)</span><br><span class="line">   .show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in python</span></span><br><span class="line">df.where(col(<span class="string">&quot;count&quot;</span>) &lt; <span class="number">2</span>).where(col(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>) != <span class="string">&quot;Croatia&quot;</span>)\</span><br><span class="line">  .show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable <span class="keyword">WHERE</span> count <span class="operator">&lt;</span> <span class="number">2</span> <span class="keyword">AND</span> ORIGIN_COUNTRY_NAME <span class="operator">!=</span> &quot;Croatia&quot;</span><br><span class="line">LIMIT <span class="number">2</span></span><br></pre></td></tr></table></figure>
<ul>
<li>将过滤器表示为SQL语句比使用编程式的DataFrame接口更简单</li>
</ul>
<h2 id="withColumn"><a class="header-anchor" href="#withColumn"></a>withColumn()</h2>
<p>添加新列</p>
<ul>
<li>包含两个参数：
<ol>
<li>列名</li>
<li>为给定行赋值的表达式</li>
</ol>
</li>
<li>还可用于对列重命名</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala or python</span></span><br><span class="line">df.withColumn(<span class="string">&quot;numberOne&quot;</span>, lit(<span class="number">1</span>)).show(<span class="number">2</span>) <span class="comment">// 添加一个仅包含数字1的列numberOne</span></span><br><span class="line"><span class="comment">// 从已有的列DEST_COUNTRY_NAME新建列Destination并删除旧列DEST_COUNTRY_NAME</span></span><br><span class="line">df.withColumn(<span class="string">&quot;Destination&quot;</span>, expr(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>))</span><br><span class="line">  .drop(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span>, <span class="number">1</span> <span class="keyword">AS</span> numberOne <span class="keyword">FROM</span> dfTable LIMIT <span class="number">2</span>;</span><br></pre></td></tr></table></figure>
<h2 id="withColumnRenamed"><a class="header-anchor" href="#withColumnRenamed"></a>withColumnRenamed()</h2>
<p>重命名列</p>
<ul>
<li>第一个参数是原始列名称</li>
<li>第二个参数是新列名称</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="comment">// 将DEST_COUNTRY_NAME重命名为dest</span></span><br><span class="line">df.withColumnRenamed(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>, <span class="string">&quot;dest&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="functions"><a class="header-anchor" href="#functions"></a>functions</h1>
<h2 id="alias"><a class="header-anchor" href="#alias"></a>alias()</h2>
<p>为选择的列起别名</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> expr, <span class="built_in">pow</span></span><br><span class="line">fabricatedQuantity = <span class="built_in">pow</span>(col(<span class="string">&quot;Quantity&quot;</span>) * col(<span class="string">&quot;UnitPrice&quot;</span>), <span class="number">2</span>) + <span class="number">5</span></span><br><span class="line">df.select(expr(<span class="string">&quot;CustomerId&quot;</span>), fabricatedQuantity.alias(<span class="string">&quot;realQuantity&quot;</span>)).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="array-contains"><a class="header-anchor" href="#array-contains"></a>array_contains()</h2>
<p>查询数组是否包含某个值</p>
<ul>
<li>返回<code>true</code>或<code>false</code></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.array_contains</span><br><span class="line">df.select(array_contains(split(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot; &quot;</span>), <span class="string">&quot;WHITE&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// 将Description按照空格拆分成数组，判断拆分后的数组是否包含字符串&quot;WHITE&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> array_contains</span><br><span class="line">df.select(array_contains(split(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot; &quot;</span>), <span class="string">&quot;WHITE&quot;</span>)).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> ARRAY_CONTAINS(SPLIT(COL(&quot;Desription&quot;), &quot; &quot;), &quot;WHITE&quot;) <span class="keyword">FROM</span> dfTable;</span><br></pre></td></tr></table></figure>
<h2 id="coalesce-2"><a class="header-anchor" href="#coalesce-2"></a>coalesce()</h2>
<p>从一组列中选择第一个非空值（第一个非NULL值）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.coalesce</span><br><span class="line">df.select(coalesce(col(<span class="string">&quot;Description&quot;</span>)， col(<span class="string">&quot;CustomerId&quot;</span>))).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> coalesce</span><br><span class="line">df.select(coalesce(col(<span class="string">&quot;Description&quot;</span>)， col(<span class="string">&quot;CustomerId&quot;</span>))).show()</span><br></pre></td></tr></table></figure>
<h2 id="col-、column"><a class="header-anchor" href="#col-、column"></a>col()、column()</h2>
<p>构造和引用列（获取指定字段）；需要传入列名</p>
<ul>
<li>返回对象为Column类型</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;col, column&#125;</span><br><span class="line">col(<span class="string">&quot;someColumnName&quot;</span>)</span><br><span class="line">column(<span class="string">&quot;someColumnName&quot;</span>)</span><br><span class="line"><span class="comment">// Scala还可使用下列方式创建列</span></span><br><span class="line">$<span class="string">&quot;myColumn&quot;</span></span><br><span class="line">&#x27;myColumn</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> col, column</span><br><span class="line">col(<span class="string">&quot;someColumnName&quot;</span>)</span><br><span class="line">column(<span class="string">&quot;someColumnName&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="collect"><a class="header-anchor" href="#collect"></a>collect()</h2>
<p>从整个DataFrame中获取所有数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">collectDF = df.limit(<span class="number">10</span>)</span><br><span class="line">collectDF.take(<span class="number">5</span>) <span class="comment"># 获取整数行</span></span><br><span class="line">collectDF.show() <span class="comment"># 更友好的打印</span></span><br><span class="line">collectDF.show(<span class="number">5</span>, <span class="literal">False</span>)</span><br><span class="line">collectDF.collect() <span class="comment"># 获取所有的数据</span></span><br></pre></td></tr></table></figure>
<h2 id="corr"><a class="header-anchor" href="#corr"></a>corr()</h2>
<p>计算两列的相关系数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;corr&#125;</span><br><span class="line">df.stat.corr(<span class="string">&quot;Quantity&quot;</span>, <span class="string">&quot;UnitPrice&quot;</span>)</span><br><span class="line">df.select(corr(<span class="string">&quot;Quantity&quot;</span>, <span class="string">&quot;UnitPrice&quot;</span>)).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> corr</span><br><span class="line">df.stat.corr(<span class="string">&quot;Quantity&quot;</span>, <span class="string">&quot;UnitPrice&quot;</span>)</span><br><span class="line">df.select(corr(<span class="string">&quot;Quantity&quot;</span>, <span class="string">&quot;UnitPrice&quot;</span>)).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">corr</span>(Quantity, UnitPrice) <span class="keyword">FROM</span> dfTable;</span><br></pre></td></tr></table></figure>
<h2 id="count"><a class="header-anchor" href="#count"></a>count</h2>
<p>统计记录条数</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// <span class="keyword">in</span> Scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="keyword">sql</span>.<span class="keyword">functions</span>.&#123;count&#125;</span><br><span class="line"></span><br><span class="line"># <span class="keyword">in</span> Python</span><br><span class="line"><span class="keyword">from</span> pyspark.<span class="keyword">sql</span>.<span class="keyword">functions</span> <span class="keyword">import</span> count</span><br></pre></td></tr></table></figure>
<h2 id="current-date"><a class="header-anchor" href="#current-date"></a>current_date()</h2>
<p>获取当前日期</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;current_date, current_timestamp&#125;</span><br><span class="line"><span class="keyword">val</span> dateDF = spark.range(<span class="number">10</span>)</span><br><span class="line">  .withColumn(<span class="string">&quot;today&quot;</span>, current_date())</span><br><span class="line">  .withColumn(<span class="string">&quot;now&quot;</span>, current_timestamp())</span><br><span class="line">dateDF.createOrReplaceTempView(<span class="string">&quot;dateTable&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> current_date, current_timestamp</span><br><span class="line">dateDF = spark.<span class="built_in">range</span>(<span class="number">10</span>)\</span><br><span class="line">  .withColumn(<span class="string">&quot;today&quot;</span>, current_date())\</span><br><span class="line">  .withColumn(<span class="string">&quot;now&quot;</span>, current_timestamp())</span><br><span class="line">dateDF.createOrReplaceTempView(<span class="string">&quot;dateTable&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="current-timestamp"><a class="header-anchor" href="#current-timestamp"></a>current_timestamp()</h2>
<p>获取当前时间戳</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;current_date, current_timestamp&#125;</span><br><span class="line"><span class="keyword">val</span> dateDF = spark.range(<span class="number">10</span>)</span><br><span class="line">  .withColumn(<span class="string">&quot;today&quot;</span>, current_date())</span><br><span class="line">  .withColumn(<span class="string">&quot;now&quot;</span>, current_timestamp())</span><br><span class="line">dateDF.createOrReplaceTempView(<span class="string">&quot;dateTable&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> current_date, current_timestamp</span><br><span class="line">dateDF = spark.<span class="built_in">range</span>(<span class="number">10</span>)\</span><br><span class="line">  .withColumn(<span class="string">&quot;today&quot;</span>, current_date())\</span><br><span class="line">  .withColumn(<span class="string">&quot;now&quot;</span>, current_timestamp())</span><br><span class="line">dateDF.createOrReplaceTempView(<span class="string">&quot;dateTable&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="date-add"><a class="header-anchor" href="#date-add"></a>date_add()</h2>
<p>添加天数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;date_add, date_sub&#125;</span><br><span class="line">dateDF.select(date_sub(col(<span class="string">&quot;today&quot;</span>), <span class="number">5</span>), date_add(col(<span class="string">&quot;today&quot;</span>), <span class="number">5</span>)).show(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> date_add, date_sub</span><br><span class="line">dateDF.select(date_sub(col(<span class="string">&quot;today&quot;</span>), <span class="number">5</span>), date_add(col(<span class="string">&quot;today&quot;</span>), <span class="number">5</span>)).show(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> DATE_SUB(today, <span class="number">5</span>), DATE_ADD(today, <span class="number">5</span>) <span class="keyword">FROM</span> dateTable;</span><br></pre></td></tr></table></figure>
<h2 id="datediff"><a class="header-anchor" href="#datediff"></a>datediff()</h2>
<p>查看两个日期之间的间隔时间（返回两个日期之间的天数）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;datediff, months_between, to_date&#125;</span><br><span class="line">dateDF.withColumn(<span class="string">&quot;week_ago&quot;</span>, date_sub(col(<span class="string">&quot;today&quot;</span>), <span class="number">7</span>))</span><br><span class="line">  .select(datediff(col(<span class="string">&quot;week_ago&quot;</span>), col(<span class="string">&quot;today&quot;</span>))).show(<span class="number">1</span>)</span><br><span class="line">dateDF.select(</span><br><span class="line">  to_date(lit(<span class="string">&quot;2016-01-01&quot;</span>)).alias(<span class="string">&quot;start&quot;</span>),</span><br><span class="line">  to_date(lit(<span class="string">&quot;2017-05-22&quot;</span>)).alias(<span class="string">&quot;end&quot;</span>))</span><br><span class="line">  .select(months_between(col(<span class="string">&quot;start&quot;</span>), col(<span class="string">&quot;end&quot;</span>))).show(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> datediff, months_between, to_date</span><br><span class="line">dateDF.withColumn(<span class="string">&quot;week_ago&quot;</span>, date_sub(col(<span class="string">&quot;today&quot;</span>), <span class="number">7</span>))\</span><br><span class="line">  .select(datediff(col(<span class="string">&quot;week_ago&quot;</span>), col(<span class="string">&quot;today&quot;</span>))).show(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">   to_date(<span class="string">&#x27;2016-01-01&#x27;</span>), </span><br><span class="line">   months_between(<span class="string">&#x27;2016-01-01&#x27;</span>, <span class="string">&#x27;2017-01-01&#x27;</span>), datediff(<span class="string">&#x27;2016-01-01&#x27;</span>, <span class="string">&#x27;2017-01-01&#x27;</span>)</span><br><span class="line"><span class="keyword">FROM</span> dateTable;</span><br></pre></td></tr></table></figure>
<h2 id="date-sub"><a class="header-anchor" href="#date-sub"></a>date_sub()</h2>
<p>减去天数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;date_add, date_sub&#125;</span><br><span class="line">dateDF.select(date_sub(col(<span class="string">&quot;today&quot;</span>), <span class="number">5</span>), date_add(col(<span class="string">&quot;today&quot;</span>), <span class="number">5</span>)).show(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> date_add, date_sub</span><br><span class="line">dateDF.select(date_sub(col(<span class="string">&quot;today&quot;</span>), <span class="number">5</span>), date_add(col(<span class="string">&quot;today&quot;</span>), <span class="number">5</span>)).show(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> DATE_SUB(today, <span class="number">5</span>), DATE_ADD(today, <span class="number">5</span>) <span class="keyword">FROM</span> dateTable;</span><br></pre></td></tr></table></figure>
<h2 id="desc"><a class="header-anchor" href="#desc"></a>desc()</h2>
<ul>
<li>降序排列</li>
<li>结合<code>sort</code>、<code>orderBy</code>使用</li>
<li><code>desc</code>函数返回的是一个Column，而不是一个字符串</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.desc</span><br><span class="line"></span><br><span class="line">flightData</span><br><span class="line">   .groupBy(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)</span><br><span class="line">   .sum(<span class="string">&quot;count&quot;</span>)</span><br><span class="line">   .withColumnRenamed(<span class="string">&quot;sum(count)&quot;</span>, <span class="string">&quot;destination_total&quot;</span>)</span><br><span class="line">   .sort(desc(<span class="string">&quot;destination_total&quot;</span>))</span><br><span class="line">   .limit(<span class="number">5</span>)</span><br><span class="line">   .show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> desc</span><br><span class="line"></span><br><span class="line">flightData\</span><br><span class="line">   .groupBy(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)\</span><br><span class="line">   .<span class="built_in">sum</span>(<span class="string">&quot;count&quot;</span>)\</span><br><span class="line">   .withColumnRenamed(<span class="string">&quot;sum(count)&quot;</span>, <span class="string">&quot;destination_total&quot;</span>)\</span><br><span class="line">   .sort(desc(<span class="string">&quot;destination_total&quot;</span>))\</span><br><span class="line">   .limit(<span class="number">5</span>)\</span><br><span class="line">   .show()</span><br></pre></td></tr></table></figure>
<p>33页插图</p>
<h2 id="describe"><a class="header-anchor" href="#describe"></a>describe()</h2>
<p>返回数值类型字段的描述性统计值（汇总统计信息）</p>
<ul>
<li>返回DataFrame对象</li>
<li>返回以下统计值：
<ul>
<li><code>count</code>：样本数</li>
<li><code>mean</code>：均值</li>
<li><code>stddev</code>：标准差</li>
<li><code>min</code>：最小值</li>
<li><code>max</code>：最大值</li>
</ul>
</li>
<li>如果某列是字符类型，则<code>mean</code>和<code>stddev</code>为<code>null</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python or Scala</span></span><br><span class="line">df.describe().show()</span><br></pre></td></tr></table></figure>
<h2 id="explode"><a class="header-anchor" href="#explode"></a>explode()</h2>
<p>为输入的数组中的每个值创建一行。如，对<code>[&quot;Hello&quot;, &quot;World&quot;], &quot;other col&quot;</code>实施<code>explode</code>后得到</p>
<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;Hello&quot;</span>, <span class="string">&quot;other col&quot;</span></span><br><span class="line"><span class="string">&quot;World&quot;</span>, <span class="string">&quot;other col&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;split, explode&#125;</span><br><span class="line">df.withColumn(<span class="string">&quot;splitted&quot;</span>, split(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot; &quot;</span>))</span><br><span class="line">  .withColumn(<span class="string">&quot;exploded&quot;</span>, explode(col(<span class="string">&quot;splitted&quot;</span>)))</span><br><span class="line">  .select(<span class="string">&quot;Description&quot;</span>, <span class="string">&quot;InvoiceNo&quot;</span>, <span class="string">&quot;exploded&quot;</span>).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> split, explode</span><br><span class="line">df.withColumn(<span class="string">&quot;splitted&quot;</span>, split(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot; &quot;</span>))\</span><br><span class="line">  .withColumn(<span class="string">&quot;exploded&quot;</span>, explode(col(<span class="string">&quot;splitted&quot;</span>)))\</span><br><span class="line">  .select(<span class="string">&quot;Description&quot;</span>, <span class="string">&quot;InvoiceNo&quot;</span>, <span class="string">&quot;exploded&quot;</span>).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">   Description, InvoiceNo, exploded</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">   <span class="keyword">SELECT</span> <span class="operator">*</span>, SPLIT(Description, &quot; &quot;) <span class="keyword">AS</span> splitted <span class="keyword">FROM</span> dfTable</span><br><span class="line">)</span><br><span class="line"><span class="keyword">LATERAL</span> <span class="keyword">VIEW</span> EXPLODE(splitted) <span class="keyword">AS</span> exploded;</span><br></pre></td></tr></table></figure>
<ul>
<li>可使用<code>explode()</code>展开map类型，将其转换为列</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line">df.select(map(col(<span class="string">&quot;Description&quot;</span>), col(<span class="string">&quot;InvoiceNo&quot;</span>)).alias(<span class="string">&quot;complex_map&quot;</span>))</span><br><span class="line">  .selectExpr(<span class="string">&quot;explode(complex_map)&quot;</span>).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">df.select(<span class="built_in">map</span>(col(<span class="string">&quot;Description&quot;</span>), col(<span class="string">&quot;InvoiceNo&quot;</span>)).alias(<span class="string">&quot;complex_map&quot;</span>))\</span><br><span class="line">  .selectExpr(<span class="string">&quot;explode(complex_map)&quot;</span>).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>展开结果如下：</p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">+--------------------+</span>------+</span><br><span class="line"><span class="section">|                 key| value|</span></span><br><span class="line"><span class="section">+--------------------+------+</span></span><br><span class="line">|WHITE HANGING HEA...|536365|</span><br><span class="line"><span class="section">| WHITE METAL LANTERN|536365|</span></span><br><span class="line"><span class="section">+--------------------+------+</span></span><br></pre></td></tr></table></figure>
<h2 id="from-json"><a class="header-anchor" href="#from-json"></a>from_json()</h2>
<p>解析JSON数据</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.from_json</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._  <span class="comment">// 加载所有的types</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> parseSchema = <span class="keyword">new</span> <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">   <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;InvoiceNo&quot;</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">   <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;Description&quot;</span>, <span class="type">StringType</span>, <span class="literal">true</span>)))</span><br><span class="line">df.selectExpr(<span class="string">&quot;(InvoiceNo, Description) as myStruct&quot;</span>)</span><br><span class="line">  .select(to_json(col(<span class="string">&quot;myStruct&quot;</span>)).alias(<span class="string">&quot;newJSON&quot;</span>))</span><br><span class="line">  .select(from_json(col(<span class="string">&quot;newJSON&quot;</span>), parseSchema), col(<span class="string">&quot;newJSON&quot;</span>)).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> from_json</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">parseSchema = StructType((</span><br><span class="line">   StructField(<span class="string">&quot;InvoiceNo&quot;</span>,StringType(),<span class="literal">True</span>),</span><br><span class="line">   StructField(<span class="string">&quot;Description&quot;</span>,StringType(),<span class="literal">True</span>)))</span><br><span class="line">df.selectExpr(<span class="string">&quot;(InvoiceNo, Description) as myStruct&quot;</span>)\</span><br><span class="line">  .select(to_json(col(<span class="string">&quot;myStruct&quot;</span>)).alias(<span class="string">&quot;newJSON&quot;</span>))\</span><br><span class="line">  .select(from_json(col(<span class="string">&quot;newJSON&quot;</span>), parseSchema), col(<span class="string">&quot;newJSON&quot;</span>)).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="get-json-object"><a class="header-anchor" href="#get-json-object"></a>get_json_object()</h2>
<p>查询JSON对象</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="comment">// 创建JSON类型的列</span></span><br><span class="line"><span class="keyword">val</span> jsonDF = spark.range(<span class="number">1</span>).selectExpr(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   &#x27;&#123;</span></span><br><span class="line"><span class="string">      &quot;myJSONKey&quot; : </span></span><br><span class="line"><span class="string">      &#123;</span></span><br><span class="line"><span class="string">         &quot;myJSONValue&quot; : [1, 2, 3]</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">   &#125;&#x27; </span></span><br><span class="line"><span class="string">   as jsonString&quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;get_json_object, json_tuple&#125;</span><br><span class="line">jsonDF.select(</span><br><span class="line">   get_json_object(col(<span class="string">&quot;jsonString&quot;</span>), <span class="string">&quot;$.myJSONKey.myJSONValue[1]&quot;</span>) as <span class="string">&quot;column&quot;</span>, <span class="comment">// 返回2</span></span><br><span class="line">   json_tuple(col(<span class="string">&quot;jsonString&quot;</span>), <span class="string">&quot;myJSONKey&quot;</span>)).show(<span class="number">2</span>)  <span class="comment">// 返回&#123;&quot;muJSONValue&quot;: [1, 2, 3]&#125;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="comment"># 创建JSON类型的列</span></span><br><span class="line">jsonDF = spark.<span class="built_in">range</span>(<span class="number">1</span>).selectExpr(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#x27;&#123;&quot;myJSONKey&quot; : &#123;&quot;myJSONValue&quot; : [1, 2, 3]&#125; &#125;&#x27; as jsonString&quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> get_json_object, json_tuple</span><br><span class="line">jsonDF.select(</span><br><span class="line">  get_json_object(col(<span class="string">&quot;jsonString&quot;</span>), <span class="string">&quot;$.myJSONKey.myJSONValue[1]&quot;</span>) <span class="keyword">as</span> <span class="string">&quot;column&quot;</span>,</span><br><span class="line">  json_tuple(col(<span class="string">&quot;jsonString&quot;</span>), <span class="string">&quot;myJSONKey&quot;</span>)).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="initcap"><a class="header-anchor" href="#initcap"></a>initcap()</h2>
<p>将给定字符串中空格分隔的每个单词首字母大写</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;initcap&#125;</span><br><span class="line">df.select(initcap(col(<span class="string">&quot;Description&quot;</span>))).show(<span class="number">2</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> initcap</span><br><span class="line">df.select(initcap(col(<span class="string">&quot;Description&quot;</span>))).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> initcap(Description) <span class="keyword">FROM</span> dfTable;</span><br></pre></td></tr></table></figure>
<h2 id="instr"><a class="header-anchor" href="#instr"></a>instr()</h2>
<p>检查在某列上是否存在某字符串</p>
<ul>
<li>在Scala中使用<code>contains()</code>函数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> instr</span><br><span class="line">containsBlack = instr(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot;BLACK&quot;</span>) &gt;= <span class="number">1</span></span><br><span class="line">containsWhite = instr(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot;WHITE&quot;</span>) &gt;= <span class="number">1</span></span><br><span class="line">df.withColumn(<span class="string">&quot;hasSimpleColor&quot;</span>, containsBlack | containsWhite)\</span><br><span class="line">  .where(<span class="string">&quot;hasSimpleColor&quot;</span>)\</span><br><span class="line">  .select(<span class="string">&quot;Description&quot;</span>).show(<span class="number">3</span>, <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> Description <span class="keyword">FROM</span> dfTable</span><br><span class="line"><span class="keyword">WHERE</span> instr(Description, <span class="string">&#x27;BLACK&#x27;</span>) <span class="operator">&gt;=</span> <span class="number">1</span> <span class="keyword">OR</span> instr(Description, <span class="string">&#x27;WHITE&#x27;</span>) <span class="operator">&gt;=</span> <span class="number">1</span>;</span><br></pre></td></tr></table></figure>
<h2 id="json-tuple"><a class="header-anchor" href="#json-tuple"></a>json_tuple()</h2>
<p>如果JSON对象只有一层嵌套，则可使用该函数进行查询</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="comment">// 创建JSON类型的列</span></span><br><span class="line"><span class="keyword">val</span> jsonDF = spark.range(<span class="number">1</span>).selectExpr(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   &#x27;&#123;</span></span><br><span class="line"><span class="string">      &quot;myJSONKey&quot; : </span></span><br><span class="line"><span class="string">      &#123;</span></span><br><span class="line"><span class="string">         &quot;myJSONValue&quot; : [1, 2, 3]</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">   &#125;&#x27; </span></span><br><span class="line"><span class="string">   as jsonString&quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;get_json_object, json_tuple&#125;</span><br><span class="line">jsonDF.select(</span><br><span class="line">   get_json_object(col(<span class="string">&quot;jsonString&quot;</span>), <span class="string">&quot;$.myJSONKey.myJSONValue[1]&quot;</span>) as <span class="string">&quot;column&quot;</span>, <span class="comment">// 返回2</span></span><br><span class="line">   json_tuple(col(<span class="string">&quot;jsonString&quot;</span>), <span class="string">&quot;myJSONKey&quot;</span>)).show(<span class="number">2</span>)  <span class="comment">// 返回&#123;&quot;muJSONValue&quot;: [1, 2, 3]&#125;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="comment"># 创建JSON类型的列</span></span><br><span class="line">jsonDF = spark.<span class="built_in">range</span>(<span class="number">1</span>).selectExpr(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#x27;&#123;&quot;myJSONKey&quot; : &#123;&quot;myJSONValue&quot; : [1, 2, 3]&#125; &#125;&#x27; as jsonString&quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> get_json_object, json_tuple</span><br><span class="line">jsonDF.select(</span><br><span class="line">  get_json_object(col(<span class="string">&quot;jsonString&quot;</span>), <span class="string">&quot;$.myJSONKey.myJSONValue[1]&quot;</span>) <span class="keyword">as</span> <span class="string">&quot;column&quot;</span>,</span><br><span class="line">  json_tuple(col(<span class="string">&quot;jsonString&quot;</span>), <span class="string">&quot;myJSONKey&quot;</span>)).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="lit"><a class="header-anchor" href="#lit"></a>lit()</h2>
<p>创造字面量（literal）（常量值）；把其他语言的类型转换为与其相对应的Spark表示</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.lit</span><br><span class="line">df.select(expr(<span class="string">&quot;*&quot;</span>), lit(<span class="number">1</span>).as(<span class="string">&quot;One&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">df.select(lit(<span class="number">5</span>)， lit(<span class="string">&quot;five&quot;</span>)， lit(<span class="number">5.0</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> lit</span><br><span class="line">df.select(expr(<span class="string">&quot;*&quot;</span>), lit(<span class="number">1</span>).alias(<span class="string">&quot;One&quot;</span>)).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span>, <span class="number">1</span> <span class="keyword">AS</span> <span class="keyword">One</span> <span class="keyword">FROM</span> dfTable LIMIT <span class="number">2</span>;</span><br></pre></td></tr></table></figure>
<h2 id="locate"><a class="header-anchor" href="#locate"></a>locate()</h2>
<p>返回整数位置（从1开始）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> expr, locate</span><br><span class="line">simpleColors = [<span class="string">&quot;black&quot;</span>, <span class="string">&quot;white&quot;</span>, <span class="string">&quot;red&quot;</span>, <span class="string">&quot;green&quot;</span>, <span class="string">&quot;blue&quot;</span>]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">color_locator</span>(<span class="params">column, color_string</span>):</span><br><span class="line">   <span class="keyword">return</span> locate(color_string.upper(), column)\</span><br><span class="line">            .cast(<span class="string">&quot;boolean&quot;</span>)\</span><br><span class="line">            .alias(<span class="string">&quot;is_&quot;</span> + color_string)</span><br><span class="line">selectedColumns = [color_locator(df.Description, c) <span class="keyword">for</span> c <span class="keyword">in</span> simpleColors]</span><br><span class="line">selectedColumns.append(expr(<span class="string">&quot;*&quot;</span>))  <span class="comment"># expr()转为Column格式</span></span><br><span class="line">df.select(*selectedColumns).where(expr(<span class="string">&quot;is_white OR is_red&quot;</span>))\</span><br><span class="line">  .select(<span class="string">&quot;Description&quot;</span>).show(<span class="number">3</span>, <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h2 id="lower"><a class="header-anchor" href="#lower"></a>lower()</h2>
<p>将字符串转为小写</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;lower, upper&#125;</span><br><span class="line">df.select(col(<span class="string">&quot;Description&quot;</span>),</span><br><span class="line">   lower(col(<span class="string">&quot;Description&quot;</span>)),</span><br><span class="line">   upper(lower(col(<span class="string">&quot;Description&quot;</span>)))).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> lower, upper</span><br><span class="line">df.select(col(<span class="string">&quot;Description&quot;</span>),</span><br><span class="line">   lower(col(<span class="string">&quot;Description&quot;</span>)),</span><br><span class="line">   upper(lower(col(<span class="string">&quot;Description&quot;</span>)))).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> Description, <span class="built_in">lower</span>(Description), <span class="built_in">Upper</span>(<span class="built_in">lower</span>(Description)) <span class="keyword">FROM</span> dfTable;</span><br></pre></td></tr></table></figure>
<h2 id="lpad"><a class="header-anchor" href="#lpad"></a>lpad()</h2>
<p>在字符串左边添加空格</p>
<ul>
<li>如果<code>lpad</code>或<code>rpad</code>方法输入的数值参数小于字符串长度，将从字符串的右侧删除字符</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;lit, rpad, lpad&#125;</span><br><span class="line">df.select(</span><br><span class="line">   lpad(lit(<span class="string">&quot;HELLO&quot;</span>), <span class="number">3</span>, <span class="string">&quot; &quot;</span>).as(<span class="string">&quot;lp&quot;</span>),</span><br><span class="line">   rpad(lit(<span class="string">&quot;HELLO&quot;</span>), <span class="number">10</span>, <span class="string">&quot; &quot;</span>).as(<span class="string">&quot;rp&quot;</span>)).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> lit, rpad, lpad</span><br><span class="line">df.select(</span><br><span class="line">   lpad(lit(<span class="string">&quot;HELLO&quot;</span>), <span class="number">3</span>, <span class="string">&quot; &quot;</span>).alias(<span class="string">&quot;lp&quot;</span>),</span><br><span class="line">   rpad(lit(<span class="string">&quot;HELLO&quot;</span>), <span class="number">10</span>, <span class="string">&quot; &quot;</span>).alias(<span class="string">&quot;rp&quot;</span>)).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">   lpad(<span class="string">&#x27;HELLOOOO &#x27;</span>, <span class="number">3</span>, <span class="string">&#x27; &#x27;</span>),</span><br><span class="line">   rpad(<span class="string">&#x27;HELLOOOO &#x27;</span>, <span class="number">10</span>, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line"><span class="keyword">FROM</span> dfTable;</span><br></pre></td></tr></table></figure>
<h2 id="map"><a class="header-anchor" href="#map"></a>map()</h2>
<p>构建两列内容的键值对映射形式</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.map</span><br><span class="line">df.select(map(col(<span class="string">&quot;Description&quot;</span>), col(<span class="string">&quot;InvoiceNo&quot;</span>)).alias(<span class="string">&quot;complex_map&quot;</span>)).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> create_map</span><br><span class="line">df.select(create_map(col(<span class="string">&quot;Description&quot;</span>), col(<span class="string">&quot;InvoiceNo&quot;</span>)).alias(<span class="string">&quot;complex_map&quot;</span>))\</span><br><span class="line">  .show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> MAP(Description, InvoiceNo) <span class="keyword">AS</span> complex_map</span><br><span class="line"><span class="keyword">FROM</span> dfTable</span><br><span class="line"><span class="keyword">WHERE</span> Description <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>可以使用正确的键（key）对键值对进行查询</li>
<li>若键（key）不存在则返回NULL</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line">df.select(map(col(<span class="string">&quot;Description&quot;</span>), col(<span class="string">&quot;InvoiceNo&quot;</span>)).alias(<span class="string">&quot;complex_map&quot;</span>))</span><br><span class="line">  .selectExpr(<span class="string">&quot;complex_map[&#x27;WHITE METAL LANTERN&#x27;]&quot;</span>).show(<span class="number">2</span>)  <span class="comment">// 查询键&#x27;WHITE METAL LANTERN&#x27;对应的值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">df.select(<span class="built_in">map</span>(col(<span class="string">&quot;Description&quot;</span>), col(<span class="string">&quot;InvoiceNo&quot;</span>)).alias(<span class="string">&quot;complex_map&quot;</span>))\</span><br><span class="line">  .selectExpr(<span class="string">&quot;complex_map[&#x27;WHITE METAL LANTERN&#x27;]&quot;</span>).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>可使用<code>explode()</code>展开map类型，将其转换为列</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line">df.select(map(col(<span class="string">&quot;Description&quot;</span>), col(<span class="string">&quot;InvoiceNo&quot;</span>)).alias(<span class="string">&quot;complex_map&quot;</span>))</span><br><span class="line">  .selectExpr(<span class="string">&quot;explode(complex_map)&quot;</span>).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">df.select(<span class="built_in">map</span>(col(<span class="string">&quot;Description&quot;</span>), col(<span class="string">&quot;InvoiceNo&quot;</span>)).alias(<span class="string">&quot;complex_map&quot;</span>))\</span><br><span class="line">  .selectExpr(<span class="string">&quot;explode(complex_map)&quot;</span>).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>展开结果如下：</p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">+--------------------+</span>------+</span><br><span class="line"><span class="section">|                 key| value|</span></span><br><span class="line"><span class="section">+--------------------+------+</span></span><br><span class="line">|WHITE HANGING HEA...|536365|</span><br><span class="line"><span class="section">| WHITE METAL LANTERN|536365|</span></span><br><span class="line"><span class="section">+--------------------+------+</span></span><br></pre></td></tr></table></figure>
<h2 id="max"><a class="header-anchor" href="#max"></a>max()</h2>
<p>最大值</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.max</span><br><span class="line"></span><br><span class="line">flightData.select(max(<span class="string">&quot;count&quot;</span>)).take(<span class="number">1</span>)</span><br><span class="line"><span class="comment">// 按照count列排序的最大值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> <span class="built_in">max</span></span><br><span class="line"></span><br><span class="line">flightData.select(<span class="built_in">max</span>(<span class="string">&quot;count&quot;</span>)).count(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="mean"><a class="header-anchor" href="#mean"></a>mean()</h2>
<p>计算均值</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// <span class="keyword">in</span> Scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="keyword">sql</span>.<span class="keyword">functions</span>.&#123;mean&#125;</span><br><span class="line"></span><br><span class="line"># <span class="keyword">in</span> Python</span><br><span class="line"><span class="keyword">from</span> pyspark.<span class="keyword">sql</span>.<span class="keyword">functions</span> <span class="keyword">import</span> mean</span><br></pre></td></tr></table></figure>
<h2 id="min"><a class="header-anchor" href="#min"></a>min</h2>
<p>最小值</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// <span class="keyword">in</span> Scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="keyword">sql</span>.<span class="keyword">functions</span>.&#123;min&#125;</span><br><span class="line"></span><br><span class="line"># <span class="keyword">in</span> Python</span><br><span class="line"><span class="keyword">from</span> pyspark.<span class="keyword">sql</span>.<span class="keyword">functions</span> <span class="keyword">import</span> min</span><br></pre></td></tr></table></figure>
<h2 id="monotonically-increasing-id"><a class="header-anchor" href="#monotonically-increasing-id"></a>monotonically_increasing_id</h2>
<p>位每行添加一个唯一的id</p>
<ul>
<li>从0开始，为每行生成一个唯一值</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.monotonically_increasing_id</span><br><span class="line">df.select(monotonically_increasing_id()).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> monotonically_increasing_id</span><br><span class="line">df.select(monotonically_increasing_id()).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="months-between"><a class="header-anchor" href="#months-between"></a>months_between()</h2>
<p>返回两个日期之间相隔的月数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;months_between, to_date&#125;</span><br><span class="line"></span><br><span class="line">dateDF.select(</span><br><span class="line">  to_date(lit(<span class="string">&quot;2016-01-01&quot;</span>)).alias(<span class="string">&quot;start&quot;</span>),</span><br><span class="line">  to_date(lit(<span class="string">&quot;2017-05-22&quot;</span>)).alias(<span class="string">&quot;end&quot;</span>))</span><br><span class="line">  .select(months_between(col(<span class="string">&quot;start&quot;</span>), col(<span class="string">&quot;end&quot;</span>))).show(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> months_between, to_date</span><br><span class="line"></span><br><span class="line">dateDF.select(</span><br><span class="line">  to_date(lit(<span class="string">&quot;2016-01-01&quot;</span>)).alias(<span class="string">&quot;start&quot;</span>),</span><br><span class="line">  to_date(lit(<span class="string">&quot;2017-05-22&quot;</span>)).alias(<span class="string">&quot;end&quot;</span>))\</span><br><span class="line">  .select(months_between(col(<span class="string">&quot;start&quot;</span>), col(<span class="string">&quot;end&quot;</span>))).show(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">   to_date(<span class="string">&#x27;2016-01-01&#x27;</span>), </span><br><span class="line">   months_between(<span class="string">&#x27;2016-01-01&#x27;</span>, <span class="string">&#x27;2017-01-01&#x27;</span>), datediff(<span class="string">&#x27;2016-01-01&#x27;</span>, <span class="string">&#x27;2017-01-01&#x27;</span>)</span><br><span class="line"><span class="keyword">FROM</span> dateTable;</span><br></pre></td></tr></table></figure>
<h2 id="pow"><a class="header-anchor" href="#pow"></a>pow()</h2>
<p>进行幂运算<br>
<code>pow(n, k)</code>：计算$n^k$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> expr, <span class="built_in">pow</span></span><br><span class="line">fabricatedQuantity = <span class="built_in">pow</span>(col(<span class="string">&quot;Quantity&quot;</span>) * col(<span class="string">&quot;UnitPrice&quot;</span>)， <span class="number">2</span>) + <span class="number">5</span></span><br><span class="line">df.select(expr(<span class="string">&quot;CustomerId&quot;</span>)， fabricatedQuantity.alias(<span class="string">&quot;realQuantity&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 或</span></span><br><span class="line">df.selectExpr(</span><br><span class="line">   <span class="string">&quot;CustomerId&quot;</span>,</span><br><span class="line">   <span class="string">&quot;(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity&quot;</span>).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> customerId, (<span class="built_in">POWER</span>((Quantity <span class="operator">*</span> UnitPrice), <span class="number">2.0</span>) <span class="operator">+</span> <span class="number">5</span>) <span class="keyword">as</span> realQuantity</span><br><span class="line"><span class="keyword">FROM</span> dfTable;</span><br></pre></td></tr></table></figure>
<h2 id="regex-extract"><a class="header-anchor" href="#regex-extract"></a>regex_extract()</h2>
<p>提取符合条件的字符串</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="comment">// 提取Description列中第一个被提到的颜色</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.regexp_extract</span><br><span class="line"><span class="keyword">val</span> simpleColors = <span class="type">Seq</span>(<span class="string">&quot;black&quot;</span>, <span class="string">&quot;white&quot;</span>, <span class="string">&quot;red&quot;</span>, <span class="string">&quot;green&quot;</span>, <span class="string">&quot;blue&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> regexString = simpleColors.map(_.toUpperCase).mkString(<span class="string">&quot;(&quot;</span>, <span class="string">&quot;|&quot;</span>, <span class="string">&quot;)&quot;</span>)</span><br><span class="line"><span class="comment">// &quot;|&quot;在正则表达式中是&quot;或&quot;的意思</span></span><br><span class="line">df.select(</span><br><span class="line">   regexp_extract(col(<span class="string">&quot;Description&quot;</span>), regexString, <span class="number">1</span>).alias(<span class="string">&quot;color_clean&quot;</span>),</span><br><span class="line">   col(<span class="string">&quot;Description&quot;</span>)).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> regexp_extract</span><br><span class="line">extract_str = <span class="string">&quot;(BLACK|WHITE|RED|GREEN|BLUE)&quot;</span></span><br><span class="line">df.select(</span><br><span class="line">   regexp_extract(col(<span class="string">&quot;Description&quot;</span>), extract_str, <span class="number">1</span>).alias(<span class="string">&quot;color_clean&quot;</span>),</span><br><span class="line">   col(<span class="string">&quot;Description&quot;</span>)).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> regexp_extract(Description, <span class="string">&#x27;(BLACK|WHITE|RED|GREEN|BLUE)&#x27;</span>, <span class="number">1</span>), Description</span><br><span class="line"><span class="keyword">FROM</span> dfTable;</span><br></pre></td></tr></table></figure>
<h2 id="regex-replace"><a class="header-anchor" href="#regex-replace"></a>regex_replace()</h2>
<p>替换符合条件的字符串</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="comment">// 将Description列中的颜色字符串BLACK|WHITE|RED|GREEN|BLUE替换为“COLOR”</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.regexp_replace</span><br><span class="line"><span class="keyword">val</span> simpleColors = <span class="type">Seq</span>(<span class="string">&quot;black&quot;</span>, <span class="string">&quot;white&quot;</span>, <span class="string">&quot;red&quot;</span>, <span class="string">&quot;green&quot;</span>, <span class="string">&quot;blue&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> regexString = simpleColors.map(_.toUpperCase).mkString(<span class="string">&quot;|&quot;</span>)</span><br><span class="line"><span class="comment">// &quot;|&quot;在正则表达式中是&quot;或&quot;的意思</span></span><br><span class="line">df.select(</span><br><span class="line">   regexp_replace(col(<span class="string">&quot;Description&quot;</span>), regexString, <span class="string">&quot;COLOR&quot;</span>).alias(<span class="string">&quot;color_clean&quot;</span>),</span><br><span class="line">   col(<span class="string">&quot;Description&quot;</span>)).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> regexp_replace</span><br><span class="line">regex_string = <span class="string">&quot;BLACK|WHITE|RED|GREEN|BLUE&quot;</span></span><br><span class="line">df.select(</span><br><span class="line">   regexp_replace(col(<span class="string">&quot;Description&quot;</span>), regex_string, <span class="string">&quot;COLOR&quot;</span>).alias(<span class="string">&quot;color_clean&quot;</span>),</span><br><span class="line">   col(<span class="string">&quot;Description&quot;</span>)).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">regexp_replace(Description, <span class="string">&#x27;BLACK|WHITE|RED|GREEN|BLUE&#x27;</span>, <span class="string">&#x27;COLOR&#x27;</span>) <span class="keyword">AS</span> color_clean, Description</span><br><span class="line"><span class="keyword">FROM</span> dfTable;</span><br></pre></td></tr></table></figure>
<h2 id="round"><a class="header-anchor" href="#round"></a>round()</h2>
<ul>
<li>四舍五入</li>
<li>默认情况下，如果恰好位于两个数字之间，<code>round</code>函数会向上取整</li>
<li><code>bround</code>函数可以向下取整</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;round， bround&#125;</span><br><span class="line">df.select(round(col(<span class="string">&quot;UnitPrice&quot;</span>)， <span class="number">1</span>).alias(<span class="string">&quot;rounded&quot;</span>)， col(<span class="string">&quot;UnitPrice&quot;</span>)).show(<span class="number">5</span>)</span><br><span class="line"><span class="comment">// 保留1位小数</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.lit</span><br><span class="line">df.select(round(lit(<span class="string">&quot;2.5&quot;</span>)), bround(lit(<span class="string">&quot;2.5&quot;</span>))).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// 输出3.0和2.0</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> lit, <span class="built_in">round</span>, bround</span><br><span class="line">df.select(<span class="built_in">round</span>(lit(<span class="string">&quot;2.5&quot;</span>)), bround(lit(<span class="string">&quot;2.5&quot;</span>))).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 输出3.0和2.0</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> round(<span class="number">2.5</span>), bround(<span class="number">2.5</span>);</span><br><span class="line"><span class="comment">-- 输出3.0和2.0</span></span><br></pre></td></tr></table></figure>
<h2 id="rpad"><a class="header-anchor" href="#rpad"></a>rpad()</h2>
<p>在字符串右边添加空格</p>
<ul>
<li>如果<code>lpad</code>或<code>rpad</code>方法输入的数值参数小于字符串长度，将从字符串的右侧删除字符</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;lit, rpad, lpad&#125;</span><br><span class="line">df.select(</span><br><span class="line">   lpad(lit(<span class="string">&quot;HELLO&quot;</span>), <span class="number">3</span>, <span class="string">&quot; &quot;</span>).as(<span class="string">&quot;lp&quot;</span>),</span><br><span class="line">   rpad(lit(<span class="string">&quot;HELLO&quot;</span>), <span class="number">10</span>, <span class="string">&quot; &quot;</span>).as(<span class="string">&quot;rp&quot;</span>)).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> lit, rpad, lpad</span><br><span class="line">df.select(</span><br><span class="line">   lpad(lit(<span class="string">&quot;HELLO&quot;</span>), <span class="number">3</span>, <span class="string">&quot; &quot;</span>).alias(<span class="string">&quot;lp&quot;</span>),</span><br><span class="line">   rpad(lit(<span class="string">&quot;HELLO&quot;</span>), <span class="number">10</span>, <span class="string">&quot; &quot;</span>).alias(<span class="string">&quot;rp&quot;</span>)).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">   lpad(<span class="string">&#x27;HELLOOOO &#x27;</span>, <span class="number">3</span>, <span class="string">&#x27; &#x27;</span>),</span><br><span class="line">   rpad(<span class="string">&#x27;HELLOOOO &#x27;</span>, <span class="number">10</span>, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line"><span class="keyword">FROM</span> dfTable;</span><br></pre></td></tr></table></figure>
<h2 id="size"><a class="header-anchor" href="#size"></a>size()</h2>
<p>查询数组的大小（长度）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.size</span><br><span class="line">df.select(size(split(col(<span class="string">&quot;Description&quot;</span>)， <span class="string">&quot; &quot;</span>))).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> size</span><br><span class="line">df.select(size(split(col(<span class="string">&quot;Description&quot;</span>)， <span class="string">&quot; &quot;</span>))).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="split"><a class="header-anchor" href="#split"></a>split()</h2>
<p>按照指定的分隔符将字符串分割成数组</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.split</span><br><span class="line">df.select(split(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot; &quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// 按照空格&quot; &quot;将Description列分割成数组</span></span><br><span class="line"><span class="comment">// 将&quot;WHITE HANGING ON&quot;分割成[WHITE, HANGING, ON]</span></span><br><span class="line"></span><br><span class="line">df.select(split(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot; &quot;</span>).alias(<span class="string">&quot;array_col&quot;</span>))</span><br><span class="line">  .selectExpr(<span class="string">&quot;array_col[0]&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// 按照空格&quot; &quot;将Description列分割成数组并选择数组的第一个元素</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> split</span><br><span class="line">df.select(split(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot; &quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">df.select(split(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot; &quot;</span>).alias(<span class="string">&quot;array_col&quot;</span>))\</span><br><span class="line">  .selectExpr(<span class="string">&quot;array_col[0]&quot;</span>).show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> SPLIT(Description, <span class="string">&#x27; &#x27;</span>) <span class="keyword">FROM</span> dfTable;</span><br></pre></td></tr></table></figure>
<h2 id="stddev-pop"><a class="header-anchor" href="#stddev-pop"></a>stddev_pop()</h2>
<p>计算标准差</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// <span class="keyword">in</span> Scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="keyword">sql</span>.<span class="keyword">functions</span>.&#123;stddev_pop&#125;</span><br><span class="line"></span><br><span class="line"># <span class="keyword">in</span> Python</span><br><span class="line"><span class="keyword">from</span> pyspark.<span class="keyword">sql</span>.<span class="keyword">functions</span> <span class="keyword">import</span> stddev_pop</span><br></pre></td></tr></table></figure>
<h2 id="struct"><a class="header-anchor" href="#struct"></a>struct()</h2>
<p>构建结构体</p>
<ul>
<li>在查询中用圆括号括起一组列来创建一个结构体</li>
</ul>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.select<span class="constructor">Expr(<span class="string">&quot;(Description, InvoiceNo) as complex&quot;</span>, <span class="string">&quot;*&quot;</span>)</span></span><br><span class="line">df.select<span class="constructor">Expr(<span class="string">&quot;struct(Description, InvoiceNo) as complex&quot;</span>, <span class="string">&quot;*&quot;</span>)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.struct</span><br><span class="line"><span class="keyword">val</span> complexDF = df.select(struct(<span class="string">&quot;Description&quot;</span>, <span class="string">&quot;InvoiceNo&quot;</span>).alias(<span class="string">&quot;complex&quot;</span>))</span><br><span class="line">complexDF.createOrReplaceTempView(<span class="string">&quot;complexDF&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> struct</span><br><span class="line">complexDF = df.select(struct(<span class="string">&quot;Description&quot;</span>, <span class="string">&quot;InvoiceNo&quot;</span>).alias(<span class="string">&quot;complex&quot;</span>))</span><br><span class="line">complexDF.createOrReplaceTempView(<span class="string">&quot;complexDF&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>使用<code>.</code>或<code>getField</code>访问结构体中的列</li>
</ul>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">complexDF<span class="selector-class">.select</span>(<span class="string">&quot;complex.Description&quot;</span>)</span><br><span class="line">complexDF<span class="selector-class">.select</span>(<span class="built_in">col</span>(<span class="string">&quot;complex&quot;</span>)<span class="selector-class">.getField</span>(<span class="string">&quot;Description&quot;</span>))  <span class="comment">// 访问结构体complexDF中的Description列</span></span><br></pre></td></tr></table></figure>
<ul>
<li>使用<code>.*</code>查询结构体中的所有值</li>
</ul>
<figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">complexDF.<span class="keyword">select</span>(<span class="string">&quot;complex.*&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="to-date"><a class="header-anchor" href="#to-date"></a>to_date()</h2>
<p>以指定的格式将字符串转换为日期数据</p>
<ul>
<li>默认格式：<code>yyyy-MM-dd</code>（年-月-日）</li>
<li>需要在Java SimpleDataFormat中指定我们想要的格式</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;months_between, to_date&#125;</span><br><span class="line"></span><br><span class="line">dateDF.select(</span><br><span class="line">  to_date(lit(<span class="string">&quot;2016-01-01&quot;</span>)).alias(<span class="string">&quot;start&quot;</span>),</span><br><span class="line">  to_date(lit(<span class="string">&quot;2017-05-22&quot;</span>)).alias(<span class="string">&quot;end&quot;</span>))</span><br><span class="line">  .select(months_between(col(<span class="string">&quot;start&quot;</span>), col(<span class="string">&quot;end&quot;</span>))).show(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dateFormat = <span class="string">&quot;yyyy-dd-MM&quot;</span>  <span class="comment">// 指定日期格式为年-日-月</span></span><br><span class="line"><span class="keyword">val</span> cleanDateDF = spark.range(<span class="number">1</span>).select(</span><br><span class="line">  to_date(lit(<span class="string">&quot;2017-12-11&quot;</span>), dateFormat).alias(<span class="string">&quot;date&quot;</span>),</span><br><span class="line">  to_date(lit(<span class="string">&quot;2017-20-12&quot;</span>), dateFormat).alias(<span class="string">&quot;date2&quot;</span>))</span><br><span class="line">cleanDateDF.createOrReplaceTempView(<span class="string">&quot;dateTable2&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> months_between, to_date</span><br><span class="line"></span><br><span class="line">dateDF.select(</span><br><span class="line">  to_date(lit(<span class="string">&quot;2016-01-01&quot;</span>)).alias(<span class="string">&quot;start&quot;</span>),</span><br><span class="line">  to_date(lit(<span class="string">&quot;2017-05-22&quot;</span>)).alias(<span class="string">&quot;end&quot;</span>))\</span><br><span class="line">  .select(months_between(col(<span class="string">&quot;start&quot;</span>), col(<span class="string">&quot;end&quot;</span>))).show(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">dateFormat = <span class="string">&quot;yyyy-dd-MM&quot;</span>  <span class="comment">## 指定日期格式为年-日-月</span></span><br><span class="line">cleanDateDF = spark.<span class="built_in">range</span>(<span class="number">1</span>).select(</span><br><span class="line">  to_date(lit(<span class="string">&quot;2017-12-11&quot;</span>), dateFormat).alias(<span class="string">&quot;date&quot;</span>),</span><br><span class="line">  to_date(lit(<span class="string">&quot;2017-20-12&quot;</span>), dateFormat).alias(<span class="string">&quot;date2”))</span></span><br><span class="line"><span class="string">cleanDateDF.createOrReplaceTempView(&quot;</span>dateTable2<span class="string">&quot;)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">   to_date(<span class="string">&#x27;2016-01-01&#x27;</span>), </span><br><span class="line">   months_between(<span class="string">&#x27;2016-01-01&#x27;</span>, <span class="string">&#x27;2017-01-01&#x27;</span>), datediff(<span class="string">&#x27;2016-01-01&#x27;</span>, <span class="string">&#x27;2017-01-01&#x27;</span>)</span><br><span class="line"><span class="keyword">FROM</span> dateTable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> to_date(<span class="type">date</span>, <span class="string">&#x27;yyyy-dd-MM&#x27;</span>), to_date(date2, <span class="string">&#x27;yyyy-dd-MM&#x27;</span>), to_date(<span class="type">date</span>)</span><br><span class="line"><span class="keyword">FROM</span> dateTable2;</span><br></pre></td></tr></table></figure>
<h2 id="to-json"><a class="header-anchor" href="#to-json"></a>to_json()</h2>
<p>将StructType转换为JSON字符串</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.to_json</span><br><span class="line">df.selectExpr(<span class="string">&quot;(InvoiceNo, Description) as myStruct&quot;</span>)</span><br><span class="line">  .select(to_json(col(<span class="string">&quot;myStruct&quot;</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> to_json</span><br><span class="line">df.selectExpr(<span class="string">&quot;(InvoiceNo, Description) as myStruct&quot;</span>)\</span><br><span class="line">  .select(to_json(col(<span class="string">&quot;myStruct&quot;</span>)))</span><br></pre></td></tr></table></figure>
<h2 id="to-timestamp"><a class="header-anchor" href="#to-timestamp"></a>to_timestamp()</h2>
<p>将字符串转换为时间戳</p>
<ul>
<li>要求指定一种格式</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.to_timestamp</span><br><span class="line"><span class="keyword">val</span> dateFormat = <span class="string">&quot;yyyy-dd-MM&quot;</span></span><br><span class="line">cleanDateDF.select(to_timestamp(col(<span class="string">&quot;date”), dateFormat)).show()  // date列的日期格式为&quot;</span>yyyy-dd-<span class="type">MM</span><span class="string">&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> to_timestamp</span><br><span class="line">dateFormat = <span class="string">&quot;yyyy-dd-MM&quot;</span></span><br><span class="line">cleanDateDF.select(to_timestamp(col(<span class="string">&quot;date&quot;</span>), dateFormat)).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> to_timestamp(<span class="type">date</span>, <span class="string">&#x27;yyyy-dd-MM&#x27;</span>), to_timestamp(date2, <span class="string">&#x27;yyyy-dd-MM&#x27;</span>)</span><br><span class="line"><span class="keyword">FROM</span> dateTable2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">CAST</span>(TO_DATE(<span class="string">&#x27;2020-01-01&#x27;</span>, <span class="string">&#x27;yyyy-dd-MM&#x27;</span>) <span class="keyword">AS</span> <span class="type">TIMESTAMP</span>);</span><br></pre></td></tr></table></figure>
<h2 id="translate"><a class="header-anchor" href="#translate"></a>translate()</h2>
<p>用给定的字符替换掉列中出现的所有该字符</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.translate</span><br><span class="line">df.select(translate(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot;LEET&quot;</span>, <span class="string">&quot;1337&quot;</span>), col(<span class="string">&quot;Description&quot;</span>))</span><br><span class="line">.show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// Description中的所有L都被替换成1，所有E都被替换成3，所有的T都被替换成7</span></span><br><span class="line"><span class="comment">// 替换前：WHITE METAL LANTERN</span></span><br><span class="line"><span class="comment">// 替换后：WHI73 M37A1 1AN73RN</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> translate</span><br><span class="line">df.select(translate(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot;LEET&quot;</span>, <span class="string">&quot;1337&quot;</span>),col(<span class="string">&quot;Description&quot;</span>))\</span><br><span class="line">  .show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">translate</span>(Description, <span class="string">&#x27;LEET&#x27;</span>, <span class="string">&#x27;1337&#x27;</span>), Description <span class="keyword">FROM</span> dfTable;</span><br></pre></td></tr></table></figure>
<h2 id="rdd"><a class="header-anchor" href="#rdd"></a>rdd</h2>
<h3 id="getNumPartitions"><a class="header-anchor" href="#getNumPartitions"></a>getNumPartitions</h3>
<p>获取分区数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line">df.rdd.getNumPartitions <span class="comment">// 1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">df.rdd.getNumPartitions() <span class="comment"># 1</span></span><br></pre></td></tr></table></figure>
<h1 id="Row"><a class="header-anchor" href="#Row"></a>Row()</h1>
<ul>
<li>手动创建Row对象，必须按照该行所属的DataFrame的列顺序来初始化Row对象</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line"><span class="comment">// 创建Row对象</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">val</span> myRow = <span class="type">Row</span>(<span class="string">&quot;Hello&quot;</span>, <span class="literal">null</span>, <span class="number">1</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in python</span></span><br><span class="line"><span class="comment"># 创建Row对象</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line">myRow = Row(<span class="string">&quot;Hello&quot;</span>, <span class="literal">None</span>, <span class="number">1</span>, <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h2 id="stat"><a class="header-anchor" href="#stat"></a>stat</h2>
<h3 id="approxQuantile"><a class="header-anchor" href="#approxQuantile"></a>approxQuantile()</h3>
<p>计算数据的精确分位数或近似分位数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="keyword">val</span> colName = <span class="string">&quot;UnitPrice&quot;</span></span><br><span class="line"><span class="keyword">val</span> quantileProbs = <span class="type">Array</span>(<span class="number">0.5</span>)</span><br><span class="line"><span class="keyword">val</span> relError = <span class="number">0.05</span></span><br><span class="line">df.stat.approxQuantile(<span class="string">&quot;UnitPrice&quot;</span>， quantileProbs， relError)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">colName = <span class="string">&quot;UnitPrice&quot;</span></span><br><span class="line">quantileProbs = [<span class="number">0.5</span>]</span><br><span class="line">relError = <span class="number">0.05</span></span><br><span class="line">df.stat.approxQuantile(<span class="string">&quot;UnitPrice&quot;</span>, quantileProbs, relError)</span><br></pre></td></tr></table></figure>
<h3 id="crosstab"><a class="header-anchor" href="#crosstab"></a>crosstab()</h3>
<p>查看交叉列表</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala or Python</span></span><br><span class="line">df.stat.crosstab(<span class="string">&quot;StockCode&quot;</span>， <span class="string">&quot;Quantity&quot;</span>).show()</span><br></pre></td></tr></table></figure>
<h3 id="freqItems"><a class="header-anchor" href="#freqItems"></a>freqItems()</h3>
<p>查看频繁项对</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line">df.stat.freqItems(<span class="type">Seq</span>(<span class="string">&quot;StockCode&quot;</span>， <span class="string">&quot;Quantity&quot;</span>)).show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in Python</span></span><br><span class="line">df.stat.freqItems([<span class="string">&quot;StockCode&quot;</span>， <span class="string">&quot;Quantity&quot;</span>]).show()</span><br></pre></td></tr></table></figure>
<h2 id="反引号"><a class="header-anchor" href="#反引号"></a>反引号</h2>
<p>当列名中包含空格或连字符等保留字时，有时需要通过使用反引号（注意是Tab键上方的反引号键，不是单引号）适当地对列名进行转义</p>
<ul>
<li><code>withColumn</code>允许使用保留字来创建列（因为withColumn的第一个参数只是新列名的字符串）</li>
<li>如果我们显式地使用字符串来引用列，则可以引用带有保留字符的类（而不用转义），这个字符串会被解释成字面值，而不是表达式</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// withColumn允许使用保留字来创建列</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.expr</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dfWithLongColName = df.withColumn(</span><br><span class="line">   <span class="string">&quot;This Long Column-Name&quot;</span>,  <span class="comment">// 因为withColumn的第一个参数只是新列名的字符串</span></span><br><span class="line">   expr(<span class="string">&quot;ORIGIN_COUNTRY_NAME)&quot;</span>)</span><br><span class="line">)</span><br><span class="line"><span class="comment">// 引用包含保留字的列时，需要进行转义</span></span><br><span class="line">dfWithLongColName.selectExpr(</span><br><span class="line">   <span class="string">&quot;`This Long Column-Name`&quot;</span>,</span><br><span class="line">   <span class="string">&quot;`This Long Column-Name` as `new col`&quot;</span>)</span><br><span class="line">.show(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h1 id="运算符"><a class="header-anchor" href="#运算符"></a>运算符</h1>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">Scala</th>
<th style="text-align:center">Python</th>
<th style="text-align:center">SQL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">不等于</td>
<td style="text-align:center">=!=</td>
<td style="text-align:center">!=</td>
<td style="text-align:center">!= 或 &lt;&gt;</td>
</tr>
</tbody>
</table>
<h2 id="不等于"><a class="header-anchor" href="#不等于"></a>不等于</h2>
<ul>
<li>Scala中的“等于”为<code>===</code>或<code>equalTo()</code>，“不等于”为<code>=!=</code>或<code>not()</code>；</li>
<li>Scala中的<code>=!=</code>不仅能比较字符串，也能比较表达式</li>
<li>Python的“不等于”为<code>!=</code></li>
<li>还可以使用下列方式（字符串形式的谓词表达式）表达“不等于”（Python或Scala都支持） <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in Scala</span></span><br><span class="line">df.where(<span class="string">&quot;InvoiceNo = 536365&quot;</span>)</span><br><span class="line">   .show(<span class="number">5</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="comment">// 或</span></span><br><span class="line">df.where(<span class="string">&quot;InvoiceNo &lt;&gt; 536365&quot;</span>)</span><br><span class="line">   .show(<span class="number">5</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="时间序列"><a class="header-anchor" href="#时间序列"></a>时间序列</h1>
<h1 id="配置"><a class="header-anchor" href="#配置"></a>配置</h1>
<h2 id="spark-conf"><a class="header-anchor" href="#spark-conf"></a>spark.conf</h2>
<h3 id="sql-shuffle-partitions"><a class="header-anchor" href="#sql-shuffle-partitions"></a>.sql.shuffle.partitions</h3>
<ul>
<li>默认情况，shuffle操作会输出200个shuffle分区</li>
</ul>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.<span class="keyword">conf</span>.<span class="keyword">set</span>(<span class="string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="string">&quot;5&quot;</span>)</span><br><span class="line"><span class="comment">// 限制shuffle输出分区的数量</span></span><br></pre></td></tr></table></figure>
<h3 id="sessionLocalTimeZone"><a class="header-anchor" href="#sessionLocalTimeZone"></a>sessionLocalTimeZone</h3>
<p>设置会话本地时区</p>
<h2 id="spark-sql"><a class="header-anchor" href="#spark-sql"></a>spark.sql</h2>
<h3 id="caseSensitive"><a class="header-anchor" href="#caseSensitive"></a>caseSensitive</h3>
<p>Spark默认不区分大小写，可以通过以下配置使Spark区分大小写</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"><span class="keyword">set</span> spark.sql.caseSensitive <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h1 id="数据类型"><a class="header-anchor" href="#数据类型"></a>数据类型</h1>
<h2 id="TimestampType"><a class="header-anchor" href="#TimestampType"></a>TimestampType</h2>
<ul>
<li>Spark的TimestampType只支持二级精度
<ul>
<li>如果要处理毫秒或微秒，需要将数据作为long类型操作才能解决该问题</li>
<li>在强制转换为TimestampType时，任何更高的精度都被删除</li>
</ul>
</li>
</ul>
<p>目前到102页</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in scala</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in python</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- in SQL</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="参考资料"><a class="header-anchor" href="#参考资料"></a>参考资料</h1>
<ul>
<li><a target="_blank" rel="noopener" href="https://book.douban.com/subject/30449649/">Spark权威指南</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/dabokele/article/details/52802150">Spark-SQL之DataFrame操作大全</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/Struggle99/article/details/103799524">Spark四大组件</a></li>
<li><a href></a></li>
</ul>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>一种用于学习Spark的免费云环境 <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

    </div>

    
    
    
        <div class="reward-container">
  <div>Thank you for your approval.</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="Skye 支付宝">
        <p>支付宝</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="Skye 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/reward2021.png" alt="Skye WeChat Bezahlung">
        <p>WeChat Bezahlung</p>
      </div>

  </div>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/images/wechat_qrcode.jpg">
            <span class="icon">
              <i class="fab fa-weixin"></i>
            </span>

            <span class="label">WeChat</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Data-Science/" rel="tag"># Data Science</a>
              <a href="/tags/Spark/" rel="tag"># Spark</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E8%B7%B5.html" rel="prev" title="数据分析 | 推荐系统实践">
      <i class="fa fa-chevron-left"></i> 数据分析 | 推荐系统实践
    </a></div>
      <div class="post-nav-item">
    <a href="/Spark-%E6%95%B0%E6%8D%AE%E6%BA%90.html" rel="next" title="Data Scientist | Spark 数据源">
      Data Scientist | Spark 数据源 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark"><span class="nav-number">1.</span> <span class="nav-text">Spark</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark%E5%9B%9B%E5%A4%A7%E7%BB%84%E4%BB%B6"><span class="nav-number">1.1.</span> <span class="nav-text">Spark四大组件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-Streaming"><span class="nav-number">1.1.1.</span> <span class="nav-text">Spark Streaming</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-SQL"><span class="nav-number">1.1.2.</span> <span class="nav-text">Spark SQL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GraphX"><span class="nav-number">1.1.3.</span> <span class="nav-text">GraphX</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MLlib"><span class="nav-number">1.1.4.</span> <span class="nav-text">MLlib</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C"><span class="nav-number">2.</span> <span class="nav-text">运行</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%A4%E4%BA%92%E5%BC%8F%E6%8E%A7%E5%88%B6%E5%8F%B0"><span class="nav-number">3.</span> <span class="nav-text">交互式控制台</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Python%E6%8E%A7%E5%88%B6%E5%8F%B0"><span class="nav-number">3.1.</span> <span class="nav-text">Python控制台</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scala%E6%8E%A7%E5%88%B6%E5%8F%B0"><span class="nav-number">3.2.</span> <span class="nav-text">Scala控制台</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SQL%E6%8E%A7%E5%88%B6%E5%8F%B0"><span class="nav-number">3.3.</span> <span class="nav-text">SQL控制台</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84"><span class="nav-number">4.</span> <span class="nav-text">基本架构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F"><span class="nav-number">4.1.</span> <span class="nav-text">应用程序</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-API"><span class="nav-number">5.</span> <span class="nav-text">Spark API</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E8%AF%AD%E8%A8%80%E6%94%AF%E6%8C%81"><span class="nav-number">5.1.</span> <span class="nav-text">多语言支持</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SparkSession"><span class="nav-number">5.2.</span> <span class="nav-text">SparkSession</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BD%AC%E6%8D%A2%E6%93%8D%E4%BD%9C"><span class="nav-number">5.3.</span> <span class="nav-text">转换操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%83%B0%E6%80%A7%E8%AF%84%E4%BC%B0"><span class="nav-number">5.3.1.</span> <span class="nav-text">惰性评估</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E4%BD%9C%E6%93%8D%E4%BD%9C"><span class="nav-number">5.4.</span> <span class="nav-text">动作操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark%E7%94%A8%E6%88%B7%E6%8E%A5%E5%8F%A3"><span class="nav-number">5.5.</span> <span class="nav-text">Spark用户接口</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%93%E6%9E%84%E5%8C%96API"><span class="nav-number">6.</span> <span class="nav-text">结构化API</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E5%AF%B9%E8%B1%A1"><span class="nav-number">6.1.</span> <span class="nav-text">核心对象</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DataFrame"><span class="nav-number">6.1.1.</span> <span class="nav-text">DataFrame</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dataset"><span class="nav-number">6.1.2.</span> <span class="nav-text">Dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SQL%E8%A1%A8%E5%92%8C%E8%A7%86%E5%9B%BE"><span class="nav-number">6.1.3.</span> <span class="nav-text">SQL表和视图</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%A7%E8%A1%8C"><span class="nav-number">6.2.</span> <span class="nav-text">执行</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E8%AE%A1%E5%88%92"><span class="nav-number">6.2.1.</span> <span class="nav-text">逻辑计划</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%A9%E7%90%86%E8%AE%A1%E5%88%92"><span class="nav-number">6.2.2.</span> <span class="nav-text">物理计划</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark%E7%B1%BB%E5%9E%8B"><span class="nav-number">7.</span> <span class="nav-text">Spark类型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Java%E7%B1%BB%E5%9E%8B"><span class="nav-number">7.1.</span> <span class="nav-text">Java类型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Python%E7%B1%BB%E5%9E%8B"><span class="nav-number">7.2.</span> <span class="nav-text">Python类型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scala%E7%B1%BB%E5%9E%8B"><span class="nav-number">7.3.</span> <span class="nav-text">Scala类型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DataFrame-2"><span class="nav-number">8.</span> <span class="nav-text">DataFrame</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%BC%8F"><span class="nav-number">8.1.</span> <span class="nav-text">模式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%97"><span class="nav-number">8.2.</span> <span class="nav-text">列</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A1%8C"><span class="nav-number">8.3.</span> <span class="nav-text">行</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="nav-number">8.4.</span> <span class="nav-text">表达式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BD%AC%E6%8D%A2%E6%93%8D%E4%BD%9C-2"><span class="nav-number">8.5.</span> <span class="nav-text">转换操作</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%87%BD%E6%95%B0"><span class="nav-number">9.</span> <span class="nav-text">函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#apply"><span class="nav-number">9.1.</span> <span class="nav-text">apply()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cast"><span class="nav-number">9.2.</span> <span class="nav-text">cast()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#coalesce"><span class="nav-number">9.3.</span> <span class="nav-text">coalesce()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#collectAsList"><span class="nav-number">9.4.</span> <span class="nav-text">collectAsList()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#columns"><span class="nav-number">9.5.</span> <span class="nav-text">columns</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#contains"><span class="nav-number">9.6.</span> <span class="nav-text">contains()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#createDataFrame"><span class="nav-number">9.7.</span> <span class="nav-text">createDataFrame()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#createOrReplaceTempView"><span class="nav-number">9.8.</span> <span class="nav-text">createOrReplaceTempView()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#def"><span class="nav-number">9.9.</span> <span class="nav-text">def</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#distinct"><span class="nav-number">9.10.</span> <span class="nav-text">distinct()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#drop"><span class="nav-number">9.11.</span> <span class="nav-text">drop()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#eqNullSafe"><span class="nav-number">9.12.</span> <span class="nav-text">eqNullSafe()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#euqalTo"><span class="nav-number">9.13.</span> <span class="nav-text">euqalTo()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#explain"><span class="nav-number">9.14.</span> <span class="nav-text">explain()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#first"><span class="nav-number">9.15.</span> <span class="nav-text">first()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#groupBy"><span class="nav-number">9.16.</span> <span class="nav-text">groupBy()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#head"><span class="nav-number">9.17.</span> <span class="nav-text">head()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#leq"><span class="nav-number">9.18.</span> <span class="nav-text">leq()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#limit"><span class="nav-number">9.19.</span> <span class="nav-text">limit()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#na"><span class="nav-number">9.20.</span> <span class="nav-text">na</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#drop-2"><span class="nav-number">9.20.1.</span> <span class="nav-text">drop()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fill"><span class="nav-number">9.20.2.</span> <span class="nav-text">fill()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#replace"><span class="nav-number">9.20.3.</span> <span class="nav-text">replace()</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#or"><span class="nav-number">9.21.</span> <span class="nav-text">or()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#orderBy"><span class="nav-number">9.22.</span> <span class="nav-text">orderBy()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#printSchema"><span class="nav-number">9.23.</span> <span class="nav-text">printSchema()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#randomSplit"><span class="nav-number">9.24.</span> <span class="nav-text">randomSplit()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#range"><span class="nav-number">9.25.</span> <span class="nav-text">range()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#read"><span class="nav-number">9.26.</span> <span class="nav-text">read</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#repartition"><span class="nav-number">9.27.</span> <span class="nav-text">repartition()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sample"><span class="nav-number">9.28.</span> <span class="nav-text">sample()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#select"><span class="nav-number">9.29.</span> <span class="nav-text">select()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#selectExpr"><span class="nav-number">9.30.</span> <span class="nav-text">selectExpr()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#show"><span class="nav-number">9.31.</span> <span class="nav-text">show()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sort"><span class="nav-number">9.32.</span> <span class="nav-text">sort()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sql"><span class="nav-number">9.33.</span> <span class="nav-text">sql()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#take"><span class="nav-number">9.34.</span> <span class="nav-text">take()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#takeAsList"><span class="nav-number">9.35.</span> <span class="nav-text">takeAsList()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#toDF"><span class="nav-number">9.36.</span> <span class="nav-text">toDF()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#toLocalIterator"><span class="nav-number">9.37.</span> <span class="nav-text">toLocalIterator()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#union"><span class="nav-number">9.38.</span> <span class="nav-text">union()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#where"><span class="nav-number">9.39.</span> <span class="nav-text">where()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#withColumn"><span class="nav-number">9.40.</span> <span class="nav-text">withColumn()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#withColumnRenamed"><span class="nav-number">9.41.</span> <span class="nav-text">withColumnRenamed()</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#functions"><span class="nav-number">10.</span> <span class="nav-text">functions</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#alias"><span class="nav-number">10.1.</span> <span class="nav-text">alias()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#array-contains"><span class="nav-number">10.2.</span> <span class="nav-text">array_contains()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#coalesce-2"><span class="nav-number">10.3.</span> <span class="nav-text">coalesce()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#col-%E3%80%81column"><span class="nav-number">10.4.</span> <span class="nav-text">col()、column()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#collect"><span class="nav-number">10.5.</span> <span class="nav-text">collect()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#corr"><span class="nav-number">10.6.</span> <span class="nav-text">corr()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#count"><span class="nav-number">10.7.</span> <span class="nav-text">count</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#current-date"><span class="nav-number">10.8.</span> <span class="nav-text">current_date()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#current-timestamp"><span class="nav-number">10.9.</span> <span class="nav-text">current_timestamp()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#date-add"><span class="nav-number">10.10.</span> <span class="nav-text">date_add()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#datediff"><span class="nav-number">10.11.</span> <span class="nav-text">datediff()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#date-sub"><span class="nav-number">10.12.</span> <span class="nav-text">date_sub()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#desc"><span class="nav-number">10.13.</span> <span class="nav-text">desc()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#describe"><span class="nav-number">10.14.</span> <span class="nav-text">describe()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#explode"><span class="nav-number">10.15.</span> <span class="nav-text">explode()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#from-json"><span class="nav-number">10.16.</span> <span class="nav-text">from_json()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#get-json-object"><span class="nav-number">10.17.</span> <span class="nav-text">get_json_object()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#initcap"><span class="nav-number">10.18.</span> <span class="nav-text">initcap()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#instr"><span class="nav-number">10.19.</span> <span class="nav-text">instr()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#json-tuple"><span class="nav-number">10.20.</span> <span class="nav-text">json_tuple()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lit"><span class="nav-number">10.21.</span> <span class="nav-text">lit()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#locate"><span class="nav-number">10.22.</span> <span class="nav-text">locate()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lower"><span class="nav-number">10.23.</span> <span class="nav-text">lower()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lpad"><span class="nav-number">10.24.</span> <span class="nav-text">lpad()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#map"><span class="nav-number">10.25.</span> <span class="nav-text">map()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#max"><span class="nav-number">10.26.</span> <span class="nav-text">max()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mean"><span class="nav-number">10.27.</span> <span class="nav-text">mean()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#min"><span class="nav-number">10.28.</span> <span class="nav-text">min</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#monotonically-increasing-id"><span class="nav-number">10.29.</span> <span class="nav-text">monotonically_increasing_id</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#months-between"><span class="nav-number">10.30.</span> <span class="nav-text">months_between()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pow"><span class="nav-number">10.31.</span> <span class="nav-text">pow()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#regex-extract"><span class="nav-number">10.32.</span> <span class="nav-text">regex_extract()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#regex-replace"><span class="nav-number">10.33.</span> <span class="nav-text">regex_replace()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#round"><span class="nav-number">10.34.</span> <span class="nav-text">round()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rpad"><span class="nav-number">10.35.</span> <span class="nav-text">rpad()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#size"><span class="nav-number">10.36.</span> <span class="nav-text">size()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#split"><span class="nav-number">10.37.</span> <span class="nav-text">split()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#stddev-pop"><span class="nav-number">10.38.</span> <span class="nav-text">stddev_pop()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#struct"><span class="nav-number">10.39.</span> <span class="nav-text">struct()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#to-date"><span class="nav-number">10.40.</span> <span class="nav-text">to_date()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#to-json"><span class="nav-number">10.41.</span> <span class="nav-text">to_json()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#to-timestamp"><span class="nav-number">10.42.</span> <span class="nav-text">to_timestamp()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#translate"><span class="nav-number">10.43.</span> <span class="nav-text">translate()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rdd"><span class="nav-number">10.44.</span> <span class="nav-text">rdd</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#getNumPartitions"><span class="nav-number">10.44.1.</span> <span class="nav-text">getNumPartitions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Row"><span class="nav-number">11.</span> <span class="nav-text">Row()</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#stat"><span class="nav-number">11.1.</span> <span class="nav-text">stat</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#approxQuantile"><span class="nav-number">11.1.1.</span> <span class="nav-text">approxQuantile()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#crosstab"><span class="nav-number">11.1.2.</span> <span class="nav-text">crosstab()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#freqItems"><span class="nav-number">11.1.3.</span> <span class="nav-text">freqItems()</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%BC%95%E5%8F%B7"><span class="nav-number">11.2.</span> <span class="nav-text">反引号</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%BF%90%E7%AE%97%E7%AC%A6"><span class="nav-number">12.</span> <span class="nav-text">运算符</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8D%E7%AD%89%E4%BA%8E"><span class="nav-number">12.1.</span> <span class="nav-text">不等于</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97"><span class="nav-number">13.</span> <span class="nav-text">时间序列</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE"><span class="nav-number">14.</span> <span class="nav-text">配置</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#spark-conf"><span class="nav-number">14.1.</span> <span class="nav-text">spark.conf</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sql-shuffle-partitions"><span class="nav-number">14.1.1.</span> <span class="nav-text">.sql.shuffle.partitions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sessionLocalTimeZone"><span class="nav-number">14.1.2.</span> <span class="nav-text">sessionLocalTimeZone</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spark-sql"><span class="nav-number">14.2.</span> <span class="nav-text">spark.sql</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#caseSensitive"><span class="nav-number">14.2.1.</span> <span class="nav-text">caseSensitive</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="nav-number">15.</span> <span class="nav-text">数据类型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#TimestampType"><span class="nav-number">15.1.</span> <span class="nav-text">TimestampType</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">16.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Skye</p>
  <div class="site-description" itemprop="description">刻苦，沉着，精进不休</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">157</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ShootingWang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ShootingWang"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-star"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Skye</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">441k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">6:41</span>
</div>


  <script src='https://unpkg.com/mermaid@/dist/mermaid.min.js'></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({theme: 'forest'});
    }
  </script>


<script src="/js/prism/clipboard.js"></script>
<script src="/js/prism/prism.js" async></script>
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>

<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>
















<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js', () => {
    new Valine(Object.assign({
      el         : '#valine-comments',
      path       : location.pathname,
    }, {"enable":true,"appId":"yFKKmB84nH4WRrBC07BTRKLC-gzGzoHsz","appKey":"EFXmIM4ipk11GLk8Stx40FYk","notify":true,"verify":false,"placeholder":"记得填写邮箱哦~这样你就能及时收到我的回复了~","avatar":"mm","meta":["nick","mail","link"],"pageSize":10,"language":null,"visitor":false,"comment_count":true,"recordIP":false,"serverURLs":null}
    ));
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
