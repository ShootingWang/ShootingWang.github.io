---
title: Machine Learning | Naive Bayes
date: 2020-05-11 12:58:39
tags: [机器学习,分类,有监督学习,算法]
categories: 机器学习
mathjax: true
math: true
hide: true
mermaid: true
---

<center>朴素贝叶斯</center>
<!--more-->

Naive Bayes

# 朴素贝叶斯
- 分类方法
- {% post_link 生成式模型-判别式模型 生成式模型 %}

> 贝叶斯学派的思想可概括为：
$$先验概率+数据=后验概率$$
——刘建平Pinard

## 贝叶斯公式
- **条件概率**公式
$$P(X|Y)=\frac{P(X,Y)}{P(Y)}=\frac{P(Y|X)P(X)}{P(Y)}$$
$$P(Y|X)=\frac{P(X,Y)}{P(X)}=\frac{P(X|Y)P(Y)}{P(X)}$$
- **全概率**公式
$$P(X)=\sum_{k}P(X|Y=Y_k)P(Y_k)$$
其中$\sum_{k}P(Y_k)=1$。
- **贝叶斯公式**
$$P(Y_k|X)=\frac{P(X|Y_k)P(Y_k)}{\sum_{k}P(X|Y=Y_k)P(Y_k)}$$

## 模型
考虑
- $n$个样本，每个样本有$p$个特征
 $$x_i=(x_{i1},\cdots, x_{ip})^T$$
 $i=1,\cdots,n$
- 有$K$个类别$C=\{C_1,\cdots,C_K\}$
- 每个样本的类别标签为$y_i \in \{C_1,\cdots,C_K\}$

从样本数据可以学习到朴素贝叶斯的先验分布$(k=1,\cdots,K)$
$$P(Y=C_k)$$
从而学习到条件概率分布
$$P(X=x|Y=C_k)=P(X_1=x_1,\cdots,X_p=x_p|Y=C_k)$$
接着用贝叶斯公式可以得到$X$和$Y$的联合分布$P(X,Y)$
\begin{equation}
  \begin{aligned}
P(X,Y=C_k)&=P(Y=C_k)P(X=x|Y=C_k)\\
&= P(Y=C_k)P(X_1=x_1,\cdots,X_p=x_p|Y=C_k)
  \end{aligned}
\end{equation}

由极大似然法可得$P(Y=C_k)$为类别$C_k$在训练集样本中出现的频数，即
$$P(Y=C_k)=\sum_{i=1}^n\frac{1}{n}I\left\{y_i=C_k\right\}$$
其中
$$I\left\{y_i=C_k\right\}=\left\{
\begin{array}{ll}
  1 & \mbox{如果}y_i=C_k\\
  0 & \mbox{如果}y_i\neq C_k
\end{array}
\right.$$

为了降低$P(X_1=x_1,\cdots,X_p=x_p|Y=C_k)$的求解难度，朴素贝叶斯方法<u>假设$X$的$p$个维度之间相互独立</u>，则有
\begin{equation}
  \begin{aligned}
&P(X_1=x_1,\cdots,X_p=x_p|Y=C_k)\\
=&P(X_1=x_1|Y=C_k)P(X_2=x_2|Y=C_k)\cdots P(X_p=x_p|Y=C_k)
  \end{aligned}
\end{equation}

### 基本假设
- 所有的$x_i$对于给定条件$y$是相互独立的
 $i=1,\cdots,p.$
 - 不是所有$x_i$相互独立，是$x_i|y$相互独立


相关问题
- [如果在模型训练过程中，由于失误操作导致训练数据中两个维度重复表示](https://www.nowcoder.com/test/question/done?tid=33437220&qid=304794#summary)，则模型效果精度降低



### 连续型特征
当特征属性是<u>连续型</u>时，通常假设其服从<u>高斯分布</u>（即正态分布）
$$g(x;\mu,\sigma)=\frac{1}{\sqrt{2\pi\sigma}}exp\{-\frac{(x-\mu)^2}{2\sigma^2}\}$$
则有
$$P(X_j=x_j|y_i)=g(x_j;\mu_{y_i},\sigma_{y_i})$$


### Laplace校准
Laplace校准 / 拉普拉斯平滑（Laplace Smoothing）：
可能会存在某个特征项划分没有出现的情况，即$P(x_j|y_i)=0$，这会导致分类器的质量降低——可使用**Laplace校准**改善

假设随机特征$z$的可能取值有$\{1,\cdots,k\}$，使用$\Phi_i=P(z=i)$参数化其分布。对于$n$个独立样本（观测集合）$\{z^{(1)},\cdots,z^{(n)}\}$，参数$\Phi_j$的极大似然估计为
$$\phi_j=\frac{\sum_{i=1}^n\mathbb{1}\{z^{(i)}=j\}}{n}$$

使用Laplace平滑进行修正：
$$\phi_j=\frac{\sum_{i=1}^n\mathbb{1}\{z^{(i)}=j\}+1}{n+k}$$
- 分子加1
 - 保证了$\Phi_j\neq0$对所有的$\Phi_j$都成立
- 分母加$k$（随机变量$z$能取到的最大值）
 - 保证了$\sum_{j=1}^k\Phi_j=1$仍然成立

### 预测
对于新的样本$x{0}=(x_1^0,\cdots,x_p^0)^T$，如何预测其所属的类别？
> 根据**最大后验概率**来判断分类

- 计算所有$K$个条件概率$P(Y=C_k|X=x^0)$
- $x^0$属于条件概率最大的那个类别

## 总结
### 优点
- 发源于古典数学理论，有稳定的分类效率
- 对小规模数据的表现很好
- 能够处理过分类问题
- 对缺失数据不太敏感
- 算法简单
- 常用于文本分类

### 缺点
- 朴素贝叶斯模型在给定输出类别的情况下假设各特征之间相互独立，但实际应用中该假设往往不成立
 - 特征数量较多或特征之间的相关性较大时，分类效果较差
 - 特征的相关性较小时，分两类效果较为良好
- 需要知道先验概率——先验概率一般基于假设
 - 可能会因为先验模型假设不当，导致预测效果不佳
- 对输入数据的表达形式很敏感（？）

## 应用
- [朴素贝叶斯分类器的应用](https://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html)
 - 病人分类
 - 性别分类
- [算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification)](https://www.cnblogs.com/leoo2sk/archive/2010/09/17/1829190.html)
 - 检测SNS社区中不真实账号
- [机器学习 第五章 朴素贝叶斯算法(Naive Bayes) 拉普拉斯平滑(Laplace smoothing) 文本分类的事件模型](https://zyzypeter.github.io/2017/07/22/machine-learning-ch5-Naive-Bayes/)
 - 文本分类问题
- [朴素贝叶斯分类器](https://wizardforcel.gitbooks.io/dm-algo-top10/content/naive-bayes.html)
 - 按照某人是否要打网球来划分天气


# 其他
- 对朴素贝叶斯的理解：
 朴素贝叶斯是在已知一些先验概率的情况下，由果索因的一种方法


# 参考资料
- [朴素贝叶斯算法原理小结](https://www.cnblogs.com/pinard/p/6069267.html)
- [算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification)](https://www.cnblogs.com/leoo2sk/archive/2010/09/17/1829190.html)
- [机器学习 第五章 朴素贝叶斯算法(Naive Bayes) 拉普拉斯平滑(Laplace smoothing) 文本分类的事件模型](https://zyzypeter.github.io/2017/07/22/machine-learning-ch5-Naive-Bayes/)