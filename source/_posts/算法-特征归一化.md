---
title: Feature Engineering | 特征缩放
date: 2020-09-11 13:53:11
tags: [机器学习,特征]
categories: 机器学习
mathjax: true
toc: true
hide: true
mermaid: true
---

<center>Feature Sacling</center>
<!--more-->

# 特征缩放
特征缩放（Feature Scaling）：将自变量或特征归一化的方法；通常属于数据挖掘中的数据预处理部分。

包括4种方法：
1. Rescaling （min-max normalization，最大最小归一化）
2. Mean normalization（均值归一化）
3. Standardization 标准化（Z-score normalization，Z-分值归一化）
4. Scaling to unit length（缩放成单位长度）

# 归一化/标准化
对数值类型的特征做归一化可以将所有的特征都统一到一个大致相同的数值区间内。

## 目标
1. 把数据变为 [0,1] 之间的小数
2. 把有量纲表达式变为无量纲表达式

## 方法
最常用的方法主要有：
1. 线性函数归一化（min-max normalization，最大最小归一化）
2. 零均值标准化


### 线性函数归一化
线性函数归一化（min-max normalization，最大最小归一化）：对原始数据进行线性变换，使结果映射到[0, 1]的范围，实现对原始数据的等比缩放。

$$x^* = \frac{x-x_{min}}{x_{max}-x_{min}}$$
其中：
- $x$是原始数据
- $x^*$是归一化后的数据
- $x_{max}$是原始数据的最大值
- $x_{min}$是原始数据的最小值

如果将数据“归一化”转换为任意区间 [a,b] 的数据，则相应的转换公式如下：
$$x^* = a + \frac{x-x_{min}}{x_{max}-x_{min}}\times(b-a)$$
此时，$\max{(x^{\*})}=b$ 且 $\min{(x^{*})}=a$。

```python
# Python
from sklearn.preprocessing import MinMaxScaler()

## 变换后的数据，最小值为0，最大值为1
min_max_scaler = MinMaxScaler()
data_minmax = min_max_scaler.fit_transform(original_data)
# 或
import numpy as np
def normalization(data):
    _range = np.max(data) - np.min(data)
    return (data - np.min(data)) / _range

## 变换后的数据，最小值为a，最大值为b
min_max_scaler = MinMaxScaler(feature_range=(a, b))
data_minmax = min_max_scaler.fit_transform(original_data)
## 等价于
## data_std = (data - data.min(axis=0)) / (data.max(axis=0) - data.min(axis=0))
## data_scaled = a + data_std * (b - a) 
```


### 零均值标准化
零均值归一化（Z-score standardization）：将数据缩放到均值为0、方差为1的数据。

$$z=\frac{x-\mu}{\sigma}$$
其中：
- $x$是原始数据
- $\mu$是原始数据的均值
- $\sigma$是原始数据的标准差

这种归一化方法要求原始数据近似服从高斯分布。

对任意数据，标准化后的数据的分布并不一定是标准正态分布；标准化后的数据取决于原始数据是什么分布

```python
# Python
from sklearn.preprocessing import scale
data_scaled = scale(data)
# 或
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler(copy=True, with_mean=True, with_std=True).fit(data)
data_scaled = scaler.transform(data)
# 或
import numpy as np
def standardization(data):
    mu = np.mean(data, axis=0)
    sigma = np.std(data, axis=0)
    return (data - mu) / sigma
```


```r
## R语言
scale(data, center = T, scale = T)
## center默认为T（True），表示数据中心化
## scale默认为T（True），表示数据标准化
```

## 作用
1. 提升模型的收敛速度
   > 如梯度下降法中，步长不至于太大，提高迭代速度
2. 提升模型的精度
   - 在一些涉及到距离计算的算法中效果显著。
   - 如某算法涉及欧式距离的计算，归一化后的数据取值范围较小，其计算出来的距离对结果的影响比归一化之前的距离对结果的影响更小，提高模型的精度
3. 归一化可以让不同维度之间的特征在数值上具有一定可比性


## 需要归一化/标准化的情况
1. 有些模型在各个维度进行不均匀伸缩后，最优解与原来不等价
   - {% post_link 算法-SVM SVM（支持向量机） %}
2. 有些模型在各个维度进行不均匀伸缩后，最优解与原来等价；但是归一化有助于加快**梯度下降法**的迭代速度、加快收敛速度，所以最好也事先将数据归一化/标准化
   - {% post_link 算法-线性回归 线性回归 %}
   - {% post_link 算法-LogisticRegression 逻辑回归 %}
   - 神经网络
3. 效果强烈依赖于特征是否归一化的模型/优化方法
   - {% post_link 算法-SVM SVM（支持向量机） %}
   - 神经网络
   - SGD（Stochastic Gradient Descent，随机梯度下降）
   - PCA（主成分分析）

## 不需要归一化/标准化的情况
1. 0/1取值的特征
  - 归一化会破坏0/1取值特征的稀疏性
2. 不受归一化影响的模型/方法
   - {% post_link 算法-DecisionTree 决策树（Decision Tree） %}
   - 最小二乘法OLS（基于平方损失）

{% note default %}
归一化对决策树模型并不适用。
以C4.5为例，决策树在进行节点分裂时，主要依据数据集D关于特征a的信息增益比，而信息增益比跟特征是否经过归一化是无关的，因为归一化并不会改变样本在特征a上的信息增益。
{% endnote %}