---
title: Machine Learning | 神经网络
date: 2020-05-18 09:14:58
tags: [机器学习,神经网络,深度学习]
categories: 机器学习
mathjax: true
toc: true
hide: true
mermaid: true
---

<center>Neural Network</center>
<!--more-->

NN
神经网络


# 神经网络

<meta name="referrer" content="no-referrer" />
{% asset_img neural-network.png 简单神经网络 %}


上图的每个圆圈都是一个神经元，每条线表示神经元之间的连接。

## 激活函数
神经网络的激活函数
### sigmoid
- 取值范围：$(0,1)$
$$sigmoid(x)=\frac{1}{1+e^{-x}}$$

### tanh
- 取值范围：$(-1,1)$
$$\tanh(x)=\frac{\sinh{x}}{\cosh{x}}=\frac{e^x-e^{-x}}{e^x+e^{-x}}=\frac{1+e^{-2x}}{1-e^{-2x}}$$

### relu
 - 线性整流函数（Rectified Linear Unit, **ReLU**）
 - 取值范围：$(0,+\infty)$
 $$f(x)=\max{(0,x)}$$
 在神经网络中为
 $$f(x)=\max{(0, w^Tx+b)}$$

# BP神经网络
反向传播神经网络（Back Propagation Neural Network）
- David Rumelhart和J.McClelland于1985年提出
- 有监督学习

BP神经网络分为2个过程：
1. 工作信号正向传递子过程
2. 误差信号反向传递子过程


> 1989年Robert Hecht-Nielsen证明**万能逼近定理**：对于任何闭区间内的一个连续函数，都可以用一个隐含层的BP网络来逼近

- 一个三层的BP网络就可以完成任意的$m$维到$n$维的映射；这三层分别是：输入层（Input Layer）、隐含层（Hidden Layer）、输出层（Output Layer）

<meta name="referrer" content="no-referrer" />
{% asset_img neural-network.png 简单神经网络 %}

- 在BP神经网络中，单个样本有$m$个输入，有$n$个输出，在输入层（Input Layer）和输出层（Output Layer）之间通常还有若干个隐含层（Hidden Layer）
  - 输入层和输出层的节点个数都是确定的，而隐含层节点个数不确定

- BP神经网络在训练数据时，可以采用**增量学习**或**批量学习**
  - 增量学习要求输入模式要有足够的随机性，对输入模式的噪声比较敏感；对于剧烈变化的输入模式，训练效果比较差；适合在线处理
  - 批量学习不存在输入模式次序问题，稳定性好，但是只适合离线处理
- 通常，BP神经网络在训练之前会对数据进行归一化处理，将数据映射到更小的区间内，比如$[0,1]$或$[-1,1]$

## 基本原理

### 正向传递子过程
假设节点$i$和节点$j$之间的权值（adjusting weight）为$w_{ij}$，节点$j$的阈值（deviation）为$b_j$，每个节点的输出值为$x_j$；每个节点的输出值是根据上层所有节点的输出值、当前节点与上层所有节点的权值、当前节点的阈值、激活函数来实现的，计算方式如下：
\begin{equation}
\begin{aligned}
S_j &= \sum_{i=0}^{m-1}w_{ij}x_i+b_j\\
x_j &= f(S_j)
\end{aligned}
\end{equation}

其中，$f(\cdot)$是激活函数，一般选择Sigmoid函数或线性函数。
- 在BP神经网络中，输入层节点没有阈值

### 反向传递子过程
- 误差信号反向传播子过程是基于Widrow-Hoff学习规则的
- BP神经网络的主要目的是：反复修正权值和阈值，使得误差函数达到最小



假设输出层的结果为$d_j$，误差函数为
$$E(w,b)=\frac{1}{2}\sum_{j=0}^{n-1}(d_j-y_j)^2$$

Widrow-Hoff学习规则是通过沿着相对误差平方和的最速下降方向，连续调整网络的权值和阈值。根据梯度下降法，权值矢量的修正正比于当前位置上$E(w,b)$的梯度，对于第$j$个输出节点有
$$\Delta w(i,j)=-\eta\frac{\partial{E(w,b)}}{\partial{w(i,j)}}$$

如果选择Sigmoid函数作为激活函数
$$f(x)=\frac{A}{1+e^{-\frac{x}{B}}}$$
则有
\begin{equation}
\begin{aligned}
f^\prime(x) &= \frac{A e^{- \frac{x}{B} } }{B(1+e^{- \frac{x}{B} } )^2}\\
&= \frac{1}{AB} \cdot \frac{A}{1+e^{- \frac{x}{B} } }\cdot \left( A- \frac{A}{1+e^{- \frac{x}{B} } } \right) \\
&= \frac{ f(x) \left[ A - f(x) \right] }{AB} \\
\end{aligned}
\end{equation}

对于$w_{ij}$有
\begin{equation}
\begin{aligned}
\frac{\partial{E(w,b)}}{\partial{w_{ij}}} &= \frac{1}{\partial{w_{ij}}}\cdot\frac{1}{2}\sum_{j=0}^{n-1}(d_j-y_j)^2 \\
&= (d_j-y_j)\cdot\frac{\partial{d_j}}{\partial{w_{ij}}} \\
&= (d_j-y_j)\cdot f^\prime(S_j)\cdot\frac{\partial{S_j}}{\partial{w_{ij}}} \\
&= (d_j-y_j)\cdot \frac{f(S_j)\left[A-f(S_j) \right]}{AB}\cdot \frac{\partial{S_j}}{\partial{w_{ij}}} \\
&= (d_j-y_j)\cdot \frac{f(S_j)\left[A-f(S_j) \right]}{AB}\cdot x_i \\
&= \delta_{ij}\cdot x_i \\
\end{aligned}
\end{equation}
其中
$$\delta_{ij}=(d_j-y_j)\cdot \frac{f(S_j)\left[A-f(S_j) \right]}{AB}$$

对$b_j$有
$$\frac{\partial{E(w,b)}}{\partial{b_j}}=\delta_{ij}$$

此即$\delta$**学习规则**，或称**Widrow-Hoff学习规则**或**纠错学习规则**（通过改变神经元之间的连接权值来减少系统实际输出和期望输出的误差）。

以上是通过对隐含层和输出层之间的权值、输出层的阈值计算调整量
- 隐含层和输出层之间的权值调整：
$$w_{ij} = w_{ij} - \eta_1\cdot \frac{\partial{E(w,b)}}{\partial{w_{ij}}} = w_{ij} - \eta_1\cdot \delta_{ij}\cdot x_i$$
- 输出层的阈值调整：
$$b_j = b_j - \eta_2 \cdot\frac{\partial{E(w,b)}}{\partial{b_j}} = b_j - \eta)2\cdot\delta_{ij}$$

下面针对输入层和隐含层之间的权值、隐含层的阈值计算调整量。

假设$w_{ki}$是输入层第$k$个节点和隐含层第$i$个节点之间的权值，则有
\begin{equation}
\begin{aligned}
\frac{\partial{E(w,b)}}{\partial{w_{ki}}} &= \frac{1}{\partial{w_{ki}}}\cdot\frac{1}{2}\sum_{j=0}^{n-1}(d_j-y_j)^2 \\
&= \sum_{j=0}^{n-1}(d_j-y_j)\cdot f^\prime(S_j)\cdot \frac{\partial{S_j}}{\partial{w_{ki}}} \\
&= \sum_{j=0}^{n-1}(d_j-y_j)\cdot f^\prime(S_j)\cdot \frac{\partial{S_j}}{\partial{x_i}}\cdot \frac{\partial{x_i}}{\partial{S_i}}\cdot \frac{\partial{S_i}}{\partial{w_{ki}}} \\
&= \sum_{j=0}^{n-1}\delta_{ij}\cdot w_{ij}\cdot \frac{f(S_i)\left[A-f(S_i) \right]}{AB}\cdot x_k \\
&= x_k\cdot \sum_{j=0}^{n-1}\delta_{ij}\cdot w_{ij}\cdot \frac{f(S_i)\left[A-f(S_i) \right]}{AB} \\
&= \delta_{ki}\cdot x_k \\
\end{aligned}
\end{equation}
其中
$$\delta_{ki}=\sum_{j=0}^{n-1}\delta_{ij}\cdot w_{ij}\cdot \frac{f(S_i)\left[A-f(S_i) \right]}{AB}$$

- 输入层和隐含层之间的权值调整：
$$w_{ki} = w_{ki} - \eta_1\cdot\frac{\partial{E(w,b)}}{\partial{w_{ki}}} = w_{ki} - \eta_1\cdot\delta_{ki}\cdot x_k$$
- 隐含层的阈值调整：
$$b_i = b_i - \eta_2\cdot\frac{\partial{E(w,b)}}{\partial{b_i}} = b_i - \eta_2\cdot\delta_{ki}$$

## 隐含层的选取
在BP神经网络中，输入层和输出层的节点个数都是确定的，而隐含层节点个数不确定。
- 隐含层节点个数的多少对神经网络的性能是有影响的

可以使用下述经验公式确定隐含层的节点个数：
$$h = \sqrt{m+n}+a$$

或者，使用下述经验公式确定隐含层的节点个数：
$$h = \log_2{m}$$
其中：
- $h$：隐含层节点数目
- $m$：输入层节点数目
- $n$：输出层节点数目
- $a$：为$1~10$之间的调节常数

## 激活函数
激活函数（activation function / transfer function）：将神经网络的输入转换为输出
- BP神经网络的激活函数要求处处可导
- BP神经网络一般用于分类或逼近问题
  - 如果用于**分类**，激活函数一般使用**Sigmoid函数**或**硬极限函数**
  - 如果用于**函数逼近**，则输出层节点用线性函数，即$f(x)=x$

当使用**Sigmoid函数**作为激活函数时，BP神经网络的输入与输出关系为：
- 输入：
$$net = z = w_1x_1+w_2x_2+\cdots+w_nx_n$$
- 输出：
$$y=f(z)=\frac{1}{1+e^{-z}}$$
- 输出的导数：
$$f^\prime(z)=\frac{1}{1+e^{-z}}-\frac{1}{(1+e^{-z})^2}=y(1-y)$$

- $z$在$[-5,0]$时，导数为正，且导数的值随着$z$的增大而增大，说明：$f(z)$在逐渐变大且变大的速度越来越快
- $z$在$[0,5]$时，导数为正，且导数的值随着$z$的增大而减小，说明：$f(z)$在逐渐变大但变大的速度越来越慢

## 缺陷
- 容易形成局部极小值而得不到全局最优值
  - 对初始权值和阈值有要求，要使得初始权值和阈值随机性足够好（可以多次随机来实现）
- 训练次数多使得学习效率低，收敛速度慢
- 隐含层的选取缺乏理论的指导
- 训练时，学习新样本有遗忘旧样本的趋势

## 改进
- 增加动量项：加速算法收敛
  - 动量因子$\alpha$一般选取$[0.1,0.8]$
$$w_{ij}=w_{ij}-\eta_1\cdot \delta_{ij}\cdot x_i+\alpha \Delta w_{ij}$$
- 自适应调节学习率
- 引入陡度因子


# 参考资料
- [图-简单神经网络](http://www.texample.net/media/tikz/examples/PDF/neural-network.pdf)
- [BP神经网络](https://blog.csdn.net/acdreamers/article/details/44657439)