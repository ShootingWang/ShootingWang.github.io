---
title: Machine Learning | 过拟合和欠拟合
date: 2020-05-15 09:20:00
tags: [算法,机器学习]
categories: 机器学习
math: true
mathjax: true
hide: true
---

<center>Overfitting & Underfitting</center>
<!--more-->

- **过拟合 / 过配**（overfitting）：学习器把训练样本自身的一些特点当作所有潜在样本都会具有的一般性质
- **欠拟合 / 欠配**（underfitting）：学习器对训练样本的一般性质尚未学好

# 过拟合
随着模型训练的进行，模型的复杂度增加，模型在训练数据集上的训练误差（training error）逐渐减小，但是在模型的复杂度达到一定程度时，模型在验证数据集上的误差反而随着模型的复杂度增加而增大，此时便发生了**过拟合**。

## 防止过拟合
- early stopping
- Data augmentation
- Regularization
- Dropout

### Early Stopping
迭代次数截断，在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。

具体做法：
在每一个Epoch[^1]结束时，计算验证数据集的准确率（accuracy），当准确率不再提高时，就停止训练。
> 一般的做法：在训练的过程中，记录到目前为止最好的验证集准确率，当连续n次Epoch没有达到最佳准确率时，则可认为准确率不再提高了，此时便可停止迭代。
即：
$$No-improvement-in-n$$
$n$是Epoch的次数，根据实际情况选取。

[^1]: 一个Epoch集：对所有的训练数据的一轮遍历


### 扩增数据集
获得更多的符合要求的数据

获取方法：
- 从数据源头采集更多数据
- 复制原有数据并加上随机噪声
- 重采样（Bootstrap）
- 根据当前数据集估计数据分布参数，使用该分布生成更多数据

### 正则化
在对目标函数/代价函数优化时，在目标函数/代价函数后添加一个正则项（惩罚项）——降低模型的复杂度

- $l1$正则
 - 使用$l1$正则可以得到稀疏的权值
- $l2$正则
 - 使用$l2$正则可以得到平滑的权值

#### $l1$正则

#### $l2$正则

### Dropout
通过修改ANN（人工神经网络）中隐藏层的神经元个数来防止ANN的过拟合

# 欠拟合

## 如何克服欠拟合
- 决策树：扩展分支
- 神经网络：增加训练轮数

# 参考资料
- [机器学习中防止过拟合的处理方法](https://blog.csdn.net/heyongluoyao8/article/details/49429629)
- [机器学习中用来防止过拟合的方法有哪些？](https://juejin.im/post/5b441583f265da0fb01854c4)
- [周志华《机器学习》](https://book.douban.com/subject/26708119/)