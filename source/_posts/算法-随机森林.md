---
title: Machine Learning | 随机森林
date: 2020-04-28 14:23:44
tags: [机器学习,分类,有监督学习,算法,集成学习,判别式模型,Bagging]
index_img: /img/random_forest.jpg
categories: 机器学习
mathjax: true
toc: true
hide: true
highlight: ##代码高亮
  enable: true
  style: 'Vs 2015'
  ## 样式选择：https://highlightjs.org/static/demo/
  copy_btn: true ##是否开启复制代码的按钮
---

<center>Random Forest</center>
<!--more-->


# 随机森林

- 随机森林是一种<u>判别分类方法</u>
- 随机森林是建立在{% post_link 决策树 %}基础上的集成学习器(一个包含多个决策树的分类器；输出结果由各个树的输出结果的众数决定)
- 是bagging的一个特殊进阶版
- 随机森林=Bagging+随机选择特征
 $$Random Forest=Bagging+Decision Tree$$
- 随机森林的弱学习器都是决策树（CART）
- 如何产生不同的决策树（Decision Tree，DT）？
 - 每棵决策树使用训练集的Bootstrap随机采样样本
 - 每个根节点（node）使用不同的特征子集
- 树的数目通常不少于500棵
- 不需要剪枝；在构建决策树时，RF的每棵决策树都最大可能地进行生长而不进行剪枝
- 概括RF包括4部分：
 1. 随机选择样本
 2. 随机选择特征
 3. 构建决策树
 4. 随机森林投票（平均）
- 每个基学习器只使用了训练集中约63.2%的样本，剩下约36.8%的样本可用作验证集来对其泛化性能进行“包外（Out of Bag）估计”

- 随机森林不需要归一化/标准化
> 概率模型（树形模型）不需要归一化。因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、随机森林

## RF vs GBDT

<table>
  <tr>
    <td></td>
    <td>随机森林RF</td>
    <td>GBDT</td>
  </tr>
  <tr>
    <td>相同点</td>
    <td colspan="2">1.都是由多棵树组成</br>2.最终的结果都是由多棵树一起决定</td>
  </tr>
  <tr>
    <td rowspan="2">子树</td>
    <td>可以是分类树或回归树</td>
    <td>只能是回归树</td>
  </tr>
  <tr>
    <td>并行生成子树</td>
    <td>串行（序列）生成子树</td>
  </tr>
  <tr>
    <td>集成方法</td>
    <td>多数投票</td>
    <td>将所有结果累加 或 加权累加</td>
  </tr>
  <tr>
    <td>异常值</td>
    <td>不敏感</td>
    <td>敏感</td>
  </tr> 
  <tr>
    <td></td>
    <td>对训练集一视同仁</td>
    <td>基于权值的弱分类器的集成</td>
  </tr> 
  <tr>
    <td></td>
    <td>减少方差</td>
    <td>减少偏差</td>
  </tr> 
  <tr>
    <td></td>
    <td>容易过拟合</td>
    <td></td>
  </tr> 
</table>





# 参考资料
