---
title: Machine Learning | 集成方法
date: 2020-05-11 13:40:19
tags: [算法,机器学习]
categories: 机器学习
math: true
mathjax: true
hide: true
index_img: /img/ml.jpg
---

<center>Ensemble Method</center>
<!--more-->

# 集成方法
- Bagging
- Boosting
- Stacking

集成学习（Ensemble Learning），基于多个学习算法的集成，提升预测结果。
- 多个弱学习器通过一定的策略结合成一个强学习器。
- 集成学习不是一个算法，是算法的门类。

> The process by which multiple models are <u>strategically generated</u> and <u>combined</u> in order to <u>better</u> solve a particular Machine Learning problem.
清华大学-数据挖掘：集成学习

如何创建多个、不同的基础分类器/基础学习器/基分类器：
- 使用不同的学习算法：DT，SVM，NN，KNN，…
- 使用不同的训练过程：
 - 不同的参数
 - 不同的训练集：bootstrap samples（有放回采样）
 - 不同的特征集
 
## 基学习器
基分类器：
- 基分类器不需要很强；强分类器可能会有很多问题
- 基分类器需要有较大的差异，差异性过小的分类器被集成没有意义
- 基分类器的精度最好略高于0.5（正确分类率）
 随着集成规模的增加，低于0.5的弱分类器会导致集成分类准确率不断下降，而高于0.5则会提升模型的准确率（趋于1）

## 结合策略
怎么将基分类器的结果结合起来？
1. 平均法
2. 投票法
3. 学习法

### 平均法
平均法（Averaging）：取所有基础学习器（基分类器）的输出结果的平均作为最终决策
$$H(x)=\sum_{i=1}^T\omega_ih_i(x)$$
其中
- 共$T$个基础学习器（弱学习器）$\{h_1,\cdots,h_T\}$
- 对任意一个样本$x$，$h_i(x)$为第$i$个学习器的分类结果
- $\omega_i$为基础学习器$h_i$的权重，$\omega_i\geq0,\ \sum_{i=1}^T\omega_i=1$
- 预测类别为$\{c_1,c_2,\cdots,c_K\}$

最简单的平均是\textbf{算数平均}：
$$\omega_i=\frac{1}{T}$$
$$H(x)=\frac{1}{T}\sum_{i=1}^Th_i(x)$$

### 投票法
投票法（Voting）：使用所有基础学习器（基分类器）结果的凸组合作为最终决策
- **相对多数投票法**（Plurality Voting）：少数服从多数；分类问题中，对样本的预测结果中票数最多的那个类别作为最终的分类类别；若获得最高票的类别不止一个，则随机选择其中一个作为最终类别
- **绝对多数投票法**（Majority Voting）：票过半数；多个基分类器对某一类别的预测结果大于总投票结果的一半，则预测结果为该类别，否则拒绝预测
 > 如：{% post_link 随机森林 随机森林Random Forest %}
- **加权投票法**（Weighted Majority Voting）：类似加权平均法；给予每个基分类器的预测结果权重，计算各类别的加权票数和，票数最高的那个类别作为最终预测类别
 > 如：AdaBoost

### 学习法
- General Combiner：Stacking
- Piecewise Combiner：RegionBoost

## 类别
集成学习的集成方法按基分类器连接方式分为两种：
1. **序列集成**（sequential ensemble）：基分类器按照顺序生成；基分类器之间存在依赖关系；通过对前面训练中错误——Boosting
2. **并行集成**（parallel ensemble）：基学习器可以并行生成；利用基学习器之间的独立性，通过平均可以显著降低分类错误——Bagging

### Bagging
- {% post_link 随机森林 随机森林Random Forest %}

Bagging（Bootstrap Aggregating）：对样本进行Bootstrap随机采样（Bootstrap sampling），得到k个训练集，对每个训练集训练一个弱学习器（弱分类器），得到k个分类器，通过一定的结合策略，得到一个强学习器。


### Boosting
- AdaBoost
- XGBoost
- LightGBM
- CatBoost
- Boosting Tree（提升树）

Boosting算法（提升方法），是一类可以用来减小监督式学习中偏差的机器学习算法；将弱分类器提升为强分类器。

- Boosting会减小在上一轮训练正确的样本的权重，增大错误样本的权重

#### AdaBoost
- 每次使用的是全部样本
- 每轮训练改变样本的权重
- 下一轮训练的目标是找到一个函数来拟合上一轮训练后的残差
- 当残差足够小或达到设置的最大迭代次数时停止训练


### Stacking
- 是Bagging的升级版
- 改进预测
- 结合策略是“学习法”
- 将训练集弱学习器（基学习器）的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终决策
- 这里称弱学习器为初级学习器，最后用于结合的学习器称为次级学习器
- 对于测试集，先用初级学习器预测一次，得到次级学习器的输入（Input），再用次级学习器预测一次，得到最终的预测结果

## Bagging vs Boosting
### 相同点
- 都是模型融合的方法
- 可以将弱分类器融合形成一个强分类器，融合之后的分类效果比最好的弱分类器效果更好

### 不同点

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th></th>
      <th>Bagging</th>
      <th>Boosting</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="2">样本选择</th>
      <td>训练集是在原始数据集中有放回抽取的</td>
      <td>每一轮的训练集不变</td>
    </tr>
    <tr>
      <td>从原始数据集中选出的各轮训练集之间相互独立</td>
      <td>每轮的训练集中每个样例在分类器中的权重发生变化;权值根据上一轮的分类结果进行调整</td>
    </tr>
    <tr>
      <th>样例权重</th>
      <td>每个样例的权重相等</td>
      <td>根据错误率不断调整样例的权重;<br>错误率越大则权重越大</td>
    </tr>
    <tr>
      <th>基分类器</th>
      <td>所有基分类器权重相等</td>
      <td>分类误差小的基分类器权重更大</td>
    </tr>
    <tr>
      <th>并行计算</th>
      <td>基分类器可以并行生成</td>
      <td>基分类器只能序列生成</td>
    </tr>
    <tr>
      <th>bias-variance</th>
      <td>减少variance</td>
      <td>减少bias</td>
    </tr>
  </tbody>
</table>
</div>

{% note success %}
为什么“Bagging是减少Variance，而Boosting是减少Bias”？
{% endnote %}
{% note default %}
1. **Bagging**对样本重采样（bootstrap），对每一轮采样得到的子样本集训练一个模型，最后对模型的结果取均值。
 - 由于子样本集的相似性及基分类器都是同种模型，所以各模型有近似相等的Bias和Variance
 - 因为$E\left\[\frac{\sum_iX_i}{n}\right]=E(X_i)$，所以bagging后的Bias和单个基分类器接近，所以不能“显著降低Bias”
 - 若各基分类器独立，则有
 $$Var\large(\frac{\sum_iX_i}{n}\large)=\frac{Var(X_i)}{n}$$
 因此可以“显著降低Variance”
 - 若各基分类器完全相同，则
 $$Var\left(\frac{\sum_iX_i}{n}\right)=Var(X_i)$$
 此时不会“降低Variance”
 - Bagging得到的各基分类器具有一定的相关性，但又不完全相同，所以可以“一定程度地降低Variance”
2. **Boosting**使用forward-stagewise（贪心法）最小化损失函数$L(y,\sum_ia_if_i(x))$
 - 在第$n$步，求解新的子模型$f(x)$及步长$a$来最小化$L(y, f_{n-1}(x)+af(x))$($f_{n-1}(x)$是前$n-1$步得到的子模型的和)
 - Boosting是在“Sequential”地最小化损失函数，其Bias逐步下降
 - Boosting的基分类器之间是强相关的，所以并不能“显著降低Variance”
{% endnote %}

# 参考资料
- [机器学习中Bagging和Boosting的区别](https://blog.csdn.net/u013709270/article/details/72553282)
- [为什么说bagging是减少variance，而boosting是减少bias?](https://www.zhihu.com/question/26760839/answer/40337791)